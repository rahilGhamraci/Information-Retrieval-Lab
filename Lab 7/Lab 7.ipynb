{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import nltk \n",
    "import math\n",
    "\n",
    "Porter = nltk.PorterStemmer()\n",
    "Lancaster = nltk.LancasterStemmer() \n",
    "\n",
    "ExpReg = nltk.RegexpTokenizer('(?:[A-Za-z]\\.)+|[A-Za-z]+[\\-@]\\d+(?:\\.\\d+)?|\\d+[A-Za-z]+|\\d+(?:[\\.\\,\\-]\\d+)?%?|\\w+(?:[\\-/]\\w+)*') # \\d : équivalent à [0-9] \n",
    "StopWords = nltk.corpus.stopwords.words('english') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rsv(query,stemming,preprocessing):\n",
    "   \n",
    "    rsv_dict = {}\n",
    "    \n",
    "    \n",
    "    \n",
    "    file_path = \"\"\n",
    "    if preprocessing == \"Split\":\n",
    "            match stemming:\n",
    "                case 'No Stemming':\n",
    "                    file_path = 'inverse_split.txt'\n",
    "                    query_terms = list(set([term for term in query.split() if term.lower() not in StopWords]))\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:              \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "                            if parts[1] in  query_terms:\n",
    "\n",
    "                                if parts[2] in rsv_dict:\n",
    "                                                                        \n",
    "                                    rsv_dict[parts[2]] += float(parts[4])\n",
    "                                else:\n",
    "\n",
    "                                    rsv_dict[parts[2]] = float(parts[4])                       \n",
    "                         \n",
    "                case 'Porter':\n",
    "                    file_path = 'inverse_split_porter.txt'\n",
    "                    query_terms = list(set([Porter.stem(term) for term in query.split() if term.lower() not in StopWords]))\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "                            if parts[1] in  query_terms:\n",
    "\n",
    "                                if parts[2] in rsv_dict:\n",
    "                                                                        \n",
    "                                    rsv_dict[parts[2]] += float(parts[4])\n",
    "                                else:\n",
    "\n",
    "                                    rsv_dict[parts[2]] = float(parts[4])\n",
    "                case 'Lancaster':\n",
    "                    file_path = 'inverse_split_lancaster.txt'\n",
    "                    query_terms = list(set([Lancaster.stem(term) for term in query.split() if term.lower() not in StopWords]))\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                         \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "                            if parts[1] in  query_terms:\n",
    "\n",
    "                                if parts[2] in rsv_dict:\n",
    "                                                                        \n",
    "                                    rsv_dict[parts[2]] += float(parts[4])\n",
    "                                else:\n",
    "\n",
    "                                    rsv_dict[parts[2]] = float(parts[4])\n",
    "    else:\n",
    "            match stemming:\n",
    "                case 'No Stemming':\n",
    "                    file_path = 'inverse_reg.txt'\n",
    "                    query_terms = list(set([term for term in ExpReg.tokenize(query) if term.lower() not in StopWords]))\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "                            if parts[1] in  query_terms:\n",
    "\n",
    "                                if parts[2] in rsv_dict:\n",
    "                                                                        \n",
    "                                    rsv_dict[parts[2]] += float(parts[4])\n",
    "                                else:\n",
    "\n",
    "                                    rsv_dict[parts[2]] = float(parts[4])\n",
    "                  \n",
    "                case 'Porter':\n",
    "                    file_path = 'inverse_reg_porter.txt'\n",
    "                    query_terms = list(set([Porter.stem(term) for term in ExpReg.tokenize(query) if term.lower() not in StopWords]))\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                         \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "\n",
    "                            if parts[1] in  query_terms:# parts[1] is the term\n",
    "\n",
    "                                if parts[2] in rsv_dict: # parts[2] is the document\n",
    "                                                                        \n",
    "                                    rsv_dict[parts[2]] += float(parts[4]) # parts[4] is the wight of the term in the document\n",
    "                                else:\n",
    "\n",
    "                                    rsv_dict[parts[2]] = float(parts[4])\n",
    "                    \n",
    "                   \n",
    "                case 'Lancaster':\n",
    "                    file_path = 'inverse_reg_lancaster.txt'\n",
    "                    query_terms = list(set([Lancaster.stem(term) for term in ExpReg.tokenize(query) if term.lower() not in StopWords]))\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "                            if parts[1] in  query_terms:\n",
    "\n",
    "                                if parts[2] in rsv_dict:\n",
    "                                                                        \n",
    "                                    rsv_dict[parts[2]] += float(parts[4])\n",
    "                                else:\n",
    "                                   \n",
    "                                   rsv_dict[parts[2]] = float(parts[4])\n",
    "\n",
    "    rsv_dict = dict(sorted(rsv_dict.items(),key=lambda item:item[1],reverse=True))\n",
    "    return rsv_dict\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'5': 0.6434526764861874,\n",
       " '2': 0.47914011693478853,\n",
       " '3': 0.4618931647855281,\n",
       " '6': 0.3217263382430937,\n",
       " '4': 0.24793200279139374,\n",
       " '1': 0.18061799739838874}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dict = rsv('Large language models (LLM)','Porter','Reg')\n",
    "my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cosine(query,stemming,preprocessing):\n",
    "   \n",
    "    som_wi = {}\n",
    "    som_wi_carre = {}\n",
    "    cosine_dict = {}\n",
    "    som_vi_squred =  0\n",
    "    \n",
    "    \n",
    "    \n",
    "    file_path = \"\"\n",
    "    if preprocessing == \"Split\":\n",
    "            match stemming:\n",
    "                case 'No Stemming':\n",
    "                    file_path = 'inverse_split.txt'\n",
    "                    query_terms = list(set([term for term in query.split() if term.lower() not in StopWords]))\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:              \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "                            if parts[1] in  query_terms:\n",
    "\n",
    "                                if parts[2] in som_wi:\n",
    "                                                                        \n",
    "                                    som_wi[parts[2]] += float(parts[4])\n",
    "                                else:\n",
    "\n",
    "                                    som_wi[parts[2]] = float(parts[4])\n",
    "                    \n",
    "                    for term in query_terms:\n",
    "                        \n",
    "                        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            \n",
    "                         \n",
    "                            lines = file.readlines()\n",
    "                            for line in lines:\n",
    "                                parts = line.split()\n",
    "\n",
    "                                if parts[1] == term:\n",
    "                                    som_vi_squred += 1\n",
    "                                    break\n",
    "                                \n",
    "                    file_path = 'descripteur_split.txt'\n",
    "                    \n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                         \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "\n",
    "                             # clculate the som of wi squared\n",
    "                            if parts[0] in som_wi_carre:\n",
    "                                                                                                           \n",
    "                                    som_wi_carre[parts[0]] += (float(parts[3]) * float(parts[3]))\n",
    "                            else:\n",
    "\n",
    "                                    som_wi_carre[parts[0]] = (float(parts[3]) * float(parts[3]))                                \n",
    "                         \n",
    "                case 'Porter':\n",
    "                    file_path = 'inverse_split_porter.txt'\n",
    "                    query_terms = list(set([Porter.stem(term) for term in query.split() if term.lower() not in StopWords]))\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "                            if parts[1] in  query_terms:\n",
    "\n",
    "                                if parts[2] in som_wi:\n",
    "                                                                        \n",
    "                                    som_wi[parts[2]] += float(parts[4])\n",
    "                                else:\n",
    "\n",
    "                                    som_wi[parts[2]] = float(parts[4])\n",
    "                    \n",
    "                    for term in query_terms:\n",
    "                        \n",
    "                        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            \n",
    "                         \n",
    "                            lines = file.readlines()\n",
    "                            for line in lines:\n",
    "                                parts = line.split()\n",
    "\n",
    "                                if parts[1] == term:\n",
    "                                    som_vi_squred += 1\n",
    "                                    break\n",
    "                                \n",
    "                    file_path = 'descripteur_split_porter.txt'\n",
    "                    \n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                         \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "\n",
    "                             # clculate the som of wi squared\n",
    "                            if parts[0] in som_wi_carre:\n",
    "                                                                                                           \n",
    "                                    som_wi_carre[parts[0]] += (float(parts[3]) * float(parts[3]))\n",
    "                            else:\n",
    "\n",
    "                                    som_wi_carre[parts[0]] = (float(parts[3]) * float(parts[3]))            \n",
    "                case 'Lancaster':\n",
    "                    file_path = 'inverse_split_lancaster.txt'\n",
    "                    query_terms = list(set([Lancaster.stem(term) for term in query.split() if term.lower() not in StopWords]))\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                         \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "                            if parts[1] in  query_terms:\n",
    "\n",
    "                                if parts[2] in som_wi:\n",
    "                                                                        \n",
    "                                    som_wi[parts[2]] += float(parts[4])\n",
    "                                else:\n",
    "\n",
    "                                    som_wi[parts[2]] = float(parts[4])\n",
    "                    \n",
    "                    for term in query_terms:\n",
    "                        \n",
    "                        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            \n",
    "                         \n",
    "                            lines = file.readlines()\n",
    "                            for line in lines:\n",
    "                                parts = line.split()\n",
    "\n",
    "                                if parts[1] == term:\n",
    "                                    som_vi_squred += 1\n",
    "                                    break\n",
    "\n",
    "                    \n",
    "                    file_path = 'descripteur_split_lancaster.txt'\n",
    "                    \n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                         \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "\n",
    "                             # clculate the som of wi squared\n",
    "                            if parts[0] in som_wi_carre:\n",
    "                                                                                                           \n",
    "                                    som_wi_carre[parts[0]] += (float(parts[3]) * float(parts[3]))\n",
    "                            else:\n",
    "\n",
    "                                    som_wi_carre[parts[0]] = (float(parts[3]) * float(parts[3]))            \n",
    "                             \n",
    "    else:\n",
    "            match stemming:\n",
    "                case 'No Stemming':\n",
    "                    file_path = 'inverse_reg.txt'\n",
    "                    query_terms = list(set([term for term in ExpReg.tokenize(query) if term.lower() not in StopWords]))\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "                            if parts[1] in  query_terms:\n",
    "\n",
    "                                if parts[2] in som_wi:\n",
    "                                                                        \n",
    "                                    som_wi[parts[2]] += float(parts[4])\n",
    "                                else:\n",
    "\n",
    "                                    som_wi[parts[2]] = float(parts[4])\n",
    "                    \n",
    "                    for term in query_terms:\n",
    "                        \n",
    "                        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            \n",
    "                         \n",
    "                            lines = file.readlines()\n",
    "                            for line in lines:\n",
    "                                parts = line.split()\n",
    "\n",
    "                                if parts[1] == term:\n",
    "                                    som_vi_squred += 1\n",
    "                                    break\n",
    "                    \n",
    "                    file_path = 'descripteur_reg.txt'\n",
    "                    \n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                         \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "\n",
    "                             # clculate the som of wi squared\n",
    "                            if parts[0] in som_wi_carre:\n",
    "                                                                                                           \n",
    "                                    som_wi_carre[parts[0]] += (float(parts[3]) * float(parts[3]))\n",
    "                            else:\n",
    "\n",
    "                                    som_wi_carre[parts[0]] = (float(parts[3]) * float(parts[3]))            \n",
    "                                \n",
    "                case 'Porter':\n",
    "                    file_path = 'inverse_reg_porter.txt'\n",
    "                    query_terms = list(set([Porter.stem(term) for term in ExpReg.tokenize(query) if term.lower() not in StopWords]))\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                         \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "\n",
    "                            if parts[1] in  query_terms:\n",
    "                                # clculate the som of wi \n",
    "                                if parts[2] in som_wi:\n",
    "                                                                        \n",
    "                                    som_wi[parts[2]] += float(parts[4])\n",
    "                                else:\n",
    "\n",
    "                                    som_wi[parts[2]] = float(parts[4])\n",
    "\n",
    "                                \n",
    "\n",
    "                    for term in query_terms:\n",
    "                        \n",
    "                        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            \n",
    "                         \n",
    "                            lines = file.readlines()\n",
    "                            for line in lines:\n",
    "                                parts = line.split()\n",
    "\n",
    "                                if parts[1] == term:\n",
    "                                    som_vi_squred += 1\n",
    "                                    break\n",
    "            \n",
    "            \n",
    "            \n",
    "                                \n",
    "\n",
    "                                \n",
    "                                \n",
    "                                \n",
    "                    file_path = 'descripteur_reg_porter.txt'\n",
    "                    \n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                         \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "\n",
    "                             # clculate the som of wi squared\n",
    "                            if parts[0] in som_wi_carre:\n",
    "                                                                                                           \n",
    "                                    som_wi_carre[parts[0]] += (float(parts[3]) * float(parts[3]))\n",
    "                            else:\n",
    "\n",
    "                                    som_wi_carre[parts[0]] = (float(parts[3]) * float(parts[3]))\n",
    "                    \n",
    "                    \n",
    "                case 'Lancaster':\n",
    "                    file_path = 'inverse_reg_lancaster.txt'\n",
    "                    query_terms = list(set([Lancaster.stem(term) for term in ExpReg.tokenize(query) if term.lower() not in StopWords]))\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "                            if parts[1] in  query_terms:\n",
    "\n",
    "                                if parts[2] in som_wi:\n",
    "                                                                        \n",
    "                                    som_wi[parts[2]] += float(parts[4])\n",
    "                                else:\n",
    "\n",
    "                                    som_wi[parts[2]] = float(parts[4])\n",
    "                    \n",
    "                    for term in query_terms:\n",
    "                        \n",
    "                        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            \n",
    "                         \n",
    "                            lines = file.readlines()\n",
    "                            for line in lines:\n",
    "                                parts = line.split()\n",
    "\n",
    "                                if parts[1] == term:\n",
    "                                    som_vi_squred += 1\n",
    "                                    break\n",
    "                                \n",
    "                    file_path = 'descripteur_reg_lancaster.txt'\n",
    "                    \n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                         \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "\n",
    "                             # clculate the som of wi squared\n",
    "                            if parts[0] in som_wi_carre:\n",
    "                                                                                                           \n",
    "                                    som_wi_carre[parts[0]] += (float(parts[3]) * float(parts[3]))\n",
    "                            else:\n",
    "\n",
    "                                    som_wi_carre[parts[0]] = (float(parts[3]) * float(parts[3]))    \n",
    "                           \n",
    "\n",
    "    \n",
    "    print('vi',som_vi_squred)\n",
    "    for doc,v in som_wi.items():\n",
    "        cosine_dict[doc] =  som_wi[doc] / ( math.sqrt(som_vi_squred) * math.sqrt(som_wi_carre[doc]))\n",
    "    \n",
    "    cosine_dict = dict(sorted(cosine_dict.items(),key=lambda item:item[1],reverse=True))\n",
    "    return cosine_dict\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vi 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'2': 0.36059314302940165,\n",
       " '5': 0.14456576390061102,\n",
       " '6': 0.1217244594306369,\n",
       " '1': 0.10828123288903387,\n",
       " '4': 0.07756421329768295,\n",
       " '3': 0.03657404343700965}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dict = cosine('Ranking documnets using LLMs','Porter','Reg')\n",
    "my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def jaccard(query,stemming,preprocessing):\n",
    "   \n",
    "    som_wi = {}\n",
    "    som_wi_carre = {}\n",
    "    jaccard_dict = {}\n",
    "    som_vi_squred = 0\n",
    "    \n",
    "    \n",
    "    \n",
    "    file_path = \"\"\n",
    "    if preprocessing == \"Split\":\n",
    "            match stemming:\n",
    "                case 'No Stemming':\n",
    "                    file_path = 'inverse_split.txt'\n",
    "                    query_terms = list(set([term for term in query.split() if term.lower() not in StopWords]))\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:              \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "                            if parts[1] in  query_terms:\n",
    "\n",
    "                                if parts[2] in som_wi:\n",
    "                                                                        \n",
    "                                    som_wi[parts[2]] += float(parts[4])\n",
    "                                else:\n",
    "\n",
    "                                    som_wi[parts[2]] = float(parts[4])\n",
    "\n",
    "                    for term in query_terms:\n",
    "                        \n",
    "                        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            \n",
    "                         \n",
    "                            lines = file.readlines()\n",
    "                            for line in lines:\n",
    "                                parts = line.split()\n",
    "\n",
    "                                if parts[1] == term:\n",
    "                                    som_vi_squred += 1\n",
    "                                    break\n",
    "                                \n",
    "                    file_path = 'descripteur_split.txt'\n",
    "                    \n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                         \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "\n",
    "                             # clculate the som of wi squared\n",
    "                            if parts[0] in som_wi_carre:\n",
    "                                                                                                           \n",
    "                                    som_wi_carre[parts[0]] += (float(parts[3]) * float(parts[3]))\n",
    "                            else:\n",
    "\n",
    "                                    som_wi_carre[parts[0]] = (float(parts[3]) * float(parts[3]))                                \n",
    "                         \n",
    "                case 'Porter':\n",
    "                    file_path = 'inverse_split_porter.txt'\n",
    "                    query_terms = list(set([Porter.stem(term) for term in query.split() if term.lower() not in StopWords]))\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "                            if parts[1] in  query_terms:\n",
    "\n",
    "                                if parts[2] in som_wi:\n",
    "                                                                        \n",
    "                                    som_wi[parts[2]] += float(parts[4])\n",
    "                                else:\n",
    "\n",
    "                                    som_wi[parts[2]] = float(parts[4])\n",
    "                    \n",
    "                    for term in query_terms:\n",
    "                        \n",
    "                        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            \n",
    "                         \n",
    "                            lines = file.readlines()\n",
    "                            for line in lines:\n",
    "                                parts = line.split()\n",
    "\n",
    "                                if parts[1] == term:\n",
    "                                    som_vi_squred += 1\n",
    "                                    break\n",
    "                    \n",
    "                    file_path = 'descripteur_split_porter.txt'\n",
    "                    \n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                         \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "\n",
    "                             # clculate the som of wi squared\n",
    "                            if parts[0] in som_wi_carre:\n",
    "                                                                                                           \n",
    "                                    som_wi_carre[parts[0]] += (float(parts[3]) * float(parts[3]))\n",
    "                            else:\n",
    "\n",
    "                                    som_wi_carre[parts[0]] = (float(parts[3]) * float(parts[3]))            \n",
    "                                \n",
    "                case 'Lancaster':\n",
    "                    file_path = 'inverse_split_lancaster.txt'\n",
    "                    query_terms = list(set([Lancaster.stem(term) for term in query.split() if term.lower() not in StopWords]))\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                         \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "                            if parts[1] in  query_terms:\n",
    "\n",
    "                                if parts[2] in som_wi:\n",
    "                                                                        \n",
    "                                    som_wi[parts[2]] += float(parts[4])\n",
    "                                else:\n",
    "\n",
    "                                    som_wi[parts[2]] = float(parts[4])\n",
    "                    \n",
    "                    for term in query_terms:\n",
    "                        \n",
    "                        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            \n",
    "                         \n",
    "                            lines = file.readlines()\n",
    "                            for line in lines:\n",
    "                                parts = line.split()\n",
    "\n",
    "                                if parts[1] == term:\n",
    "                                    som_vi_squred += 1\n",
    "                                    break\n",
    "                                \n",
    "                    file_path = 'descripteur_split_lancaster.txt'\n",
    "                    \n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                         \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "\n",
    "                             # clculate the som of wi squared\n",
    "                            if parts[0] in som_wi_carre:\n",
    "                                                                                                           \n",
    "                                    som_wi_carre[parts[0]] += (float(parts[3]) * float(parts[3]))\n",
    "                            else:\n",
    "\n",
    "                                    som_wi_carre[parts[0]] = (float(parts[3]) * float(parts[3]))            \n",
    "    else:\n",
    "            match stemming:\n",
    "                case 'No Stemming':\n",
    "                    file_path = 'inverse_reg.txt'\n",
    "                    query_terms = list(set([term for term in ExpReg.tokenize(query) if term.lower() not in StopWords]))\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "                            if parts[1] in  query_terms:\n",
    "\n",
    "                                if parts[2] in som_wi:\n",
    "                                                                        \n",
    "                                    som_wi[parts[2]] += float(parts[4])\n",
    "                                else:\n",
    "\n",
    "                                    som_wi[parts[2]] = float(parts[4])\n",
    "                    for term in query_terms:\n",
    "                        \n",
    "                        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            \n",
    "                         \n",
    "                            lines = file.readlines()\n",
    "                            for line in lines:\n",
    "                                parts = line.split()\n",
    "\n",
    "                                if parts[1] == term:\n",
    "                                    som_vi_squred += 1\n",
    "                                    break\n",
    "                                \n",
    "                    file_path = 'descripteur_reg.txt'\n",
    "                    \n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                         \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "\n",
    "                             # clculate the som of wi squared\n",
    "                            if parts[0] in som_wi_carre:\n",
    "                                                                                                           \n",
    "                                    som_wi_carre[parts[0]] += (float(parts[3]) * float(parts[3]))\n",
    "                            else:\n",
    "\n",
    "                                    som_wi_carre[parts[0]] = (float(parts[3]) * float(parts[3]))            \n",
    "                case 'Porter':\n",
    "                    file_path = 'inverse_reg_porter.txt'\n",
    "                    query_terms = list(set([Porter.stem(term) for term in ExpReg.tokenize(query) if term.lower() not in StopWords]))\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                         \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "\n",
    "                            if parts[1] in  query_terms:\n",
    "                                # clculate the som of wi \n",
    "                                if parts[2] in som_wi:\n",
    "                                                                        \n",
    "                                    som_wi[parts[2]] += float(parts[4])\n",
    "                                else:\n",
    "\n",
    "                                    som_wi[parts[2]] = float(parts[4])\n",
    "                    \n",
    "                    for term in query_terms:\n",
    "                        \n",
    "                        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            \n",
    "                         \n",
    "                            lines = file.readlines()\n",
    "                            for line in lines:\n",
    "                                parts = line.split()\n",
    "\n",
    "                                if parts[1] == term:\n",
    "                                    som_vi_squred += 1\n",
    "                                    break\n",
    "                                \n",
    "                    file_path = 'descripteur_reg_porter.txt'\n",
    "                    \n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                         \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "\n",
    "                            \n",
    "\n",
    "                             # clculate the som of wi squared\n",
    "                            if parts[0] in som_wi_carre:\n",
    "                                                                                                           \n",
    "                                    som_wi_carre[parts[0]] += (float(parts[3]) * float(parts[3]))\n",
    "                            else:\n",
    "\n",
    "                                    som_wi_carre[parts[0]] = (float(parts[3]) * float(parts[3]))\n",
    "\n",
    "                                    \n",
    "                    \n",
    "                    \n",
    "                case 'Lancaster':\n",
    "                    file_path = 'inverse_reg_lancaster.txt'\n",
    "                    query_terms = list(set([Lancaster.stem(term) for term in ExpReg.tokenize(query) if term.lower() not in StopWords]))\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "                            if parts[1] in  query_terms:\n",
    "\n",
    "                                if parts[2] in som_wi:\n",
    "                                                                        \n",
    "                                    som_wi[parts[2]] += float(parts[4])\n",
    "                                else:\n",
    "\n",
    "                                    som_wi[parts[2]] = float(parts[4])\n",
    "\n",
    "                    for term in query_terms:\n",
    "                        \n",
    "                        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            \n",
    "                         \n",
    "                            lines = file.readlines()\n",
    "                            for line in lines:\n",
    "                                parts = line.split()\n",
    "\n",
    "                                if parts[1] == term:\n",
    "                                    som_vi_squred += 1\n",
    "                                    break\n",
    "                                \n",
    "                    file_path = 'descripteur_reg_lancaster.txt'\n",
    "                    \n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                         \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "\n",
    "                             # clculate the som of wi squared\n",
    "                            if parts[0] in som_wi_carre:\n",
    "                                                                                                           \n",
    "                                    som_wi_carre[parts[0]] += (float(parts[3]) * float(parts[3]))\n",
    "                            else:\n",
    "\n",
    "                                    som_wi_carre[parts[0]] = (float(parts[3]) * float(parts[3]))            \n",
    "\n",
    "   \n",
    "    \n",
    "    for doc,v in som_wi.items():\n",
    "        jaccard_dict[doc] =  som_wi[doc] / ( som_vi_squred + som_wi_carre[doc] - som_wi[doc])\n",
    "    \n",
    "    jaccard_dict = dict(sorted(jaccard_dict.items(),key=lambda item:item[1],reverse=True))\n",
    "    return jaccard_dict\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2': 0.21982563356714666,\n",
       " '5': 0.07780405871287085,\n",
       " '6': 0.06347912118488071,\n",
       " '1': 0.05723930538097438,\n",
       " '4': 0.03683976214683828,\n",
       " '3': 0.018058171658908724}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dict = jaccard('Ranking documnets using LLMs','Porter','Reg')\n",
    "my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bm25(query,stemming,preprocessing,K,B):\n",
    "   \n",
    "    dl = {}\n",
    "    avdl = 0\n",
    "    freq = []\n",
    "    n = {}\n",
    "    bm25_dict = {}\n",
    "    N = 6\n",
    "    \n",
    "    #initialization of the frequencies dictionnaries of each document \n",
    "    for i in range(6):\n",
    "         freq.append({})\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    file_path = \"\"\n",
    "    if preprocessing == \"Split\":\n",
    "            match stemming:\n",
    "                case 'No Stemming':\n",
    "                    file_path = 'inverse_split.txt'\n",
    "                    query_terms = list(set([term for term in query.split() if term.lower() not in StopWords]))\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:              \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "                            if parts[1] in  query_terms:\n",
    "                                # calculate the sum of frequncies of the term  \"parts[1]\" in each document \"parts[2]\"\n",
    "                                if parts[1] not in freq[int(parts[2])-1]:\n",
    "                                                                        \n",
    "                                    freq[int(parts[2])-1][parts[1]] = int(parts[3])\n",
    "                                \n",
    "\n",
    "                                # calculating the number of documents in which term parts[1] appear\n",
    "                                if parts[1]  in n:\n",
    "                                                                        \n",
    "                                    n[parts[1]] +=  1\n",
    "                                \n",
    "                                else:\n",
    "                                    n[parts[1]] =  1\n",
    "                                \n",
    "                    file_path = 'descripteur_split.txt'\n",
    "                    \n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                         \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "                            \n",
    "                            avdl += int(parts[2])\n",
    "                            \n",
    "\n",
    "                            # calculating the sum of frequencies for each document \n",
    "                            if parts[0] in dl:\n",
    "                                                                                                           \n",
    "                                    dl[parts[0]] += int(parts[2])\n",
    "                            else:\n",
    "\n",
    "                                    dl[parts[0]] = int(parts[2])                                \n",
    "                         \n",
    "                case 'Porter':\n",
    "                    file_path = 'inverse_split_porter.txt'\n",
    "                    query_terms = list(set([Porter.stem(term) for term in query.split() if term.lower() not in StopWords]))\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "                            if parts[1] in  query_terms:\n",
    "                                # calculate the sum of frequncies of the term  \"parts[1]\" in each document \"parts[2]\"\n",
    "                                if parts[1] not in freq[int(parts[2])-1]:\n",
    "                                                                        \n",
    "                                    freq[int(parts[2])-1][parts[1]] = int(parts[3])\n",
    "                                \n",
    "\n",
    "                                # calculating the number of documents in which term parts[1] appear\n",
    "                                if parts[1]  in n:\n",
    "                                                                        \n",
    "                                    n[parts[1]] +=  1\n",
    "                                \n",
    "                                else:\n",
    "                                    n[parts[1]] =  1\n",
    "                    \n",
    "                    file_path = 'descripteur_split_porter.txt'\n",
    "                    \n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                         \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "                            \n",
    "                            avdl += int(parts[2])\n",
    "                            \n",
    "\n",
    "                            # calculating the sum of frequencies for each document \n",
    "                            if parts[0] in dl:\n",
    "                                                                                                           \n",
    "                                    dl[parts[0]] += int(parts[2])\n",
    "                            else:\n",
    "\n",
    "                                    dl[parts[0]] = int(parts[2])            \n",
    "                                \n",
    "                case 'Lancaster':\n",
    "                    file_path = 'inverse_split_lancaster.txt'\n",
    "                    query_terms = list(set([Lancaster.stem(term) for term in query.split() if term.lower() not in StopWords]))\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                         \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "                            if parts[1] in  query_terms:\n",
    "                                # calculate the sum of frequncies of the term  \"parts[1]\" in each document \"parts[2]\"\n",
    "                                if parts[1] not in freq[int(parts[2])-1]:\n",
    "                                                                        \n",
    "                                    freq[int(parts[2])-1][parts[1]] = int(parts[3])\n",
    "                                \n",
    "\n",
    "                                # calculating the number of documents in which term parts[1] appear\n",
    "                                if parts[1]  in n:\n",
    "                                                                        \n",
    "                                    n[parts[1]] +=  1\n",
    "                                \n",
    "                                else:\n",
    "                                    n[parts[1]] =  1\n",
    "                                \n",
    "                    file_path = 'descripteur_split_lancaster.txt'\n",
    "                    \n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                         \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "                            \n",
    "                            avdl += int(parts[2])\n",
    "                            \n",
    "\n",
    "                            # calculating the sum of frequencies for each document \n",
    "                            if parts[0] in dl:\n",
    "                                                                                                           \n",
    "                                    dl[parts[0]] += int(parts[2])\n",
    "                            else:\n",
    "\n",
    "                                    dl[parts[0]] = int(parts[2])            \n",
    "    else:\n",
    "            match stemming:\n",
    "                case 'No Stemming':\n",
    "                    file_path = 'inverse_reg.txt'\n",
    "                    query_terms = list(set([term for term in ExpReg.tokenize(query) if term.lower() not in StopWords]))\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "                            if parts[1] in  query_terms:\n",
    "                                # calculate the sum of frequncies of the term  \"parts[1]\" in each document \"parts[2]\"\n",
    "                                if parts[1] not in freq[int(parts[2])-1]:\n",
    "                                                                        \n",
    "                                    freq[int(parts[2])-1][parts[1]] = int(parts[3])\n",
    "                                \n",
    "\n",
    "                                # calculating the number of documents in which term parts[1] appear\n",
    "                                if parts[1]  in n:\n",
    "                                                                        \n",
    "                                    n[parts[1]] +=  1\n",
    "                                \n",
    "                                else:\n",
    "                                    n[parts[1]] =  1\n",
    "                                \n",
    "                    file_path = 'descripteur_reg.txt'\n",
    "                    \n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                         \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "                            \n",
    "                            avdl += int(parts[2])\n",
    "                            \n",
    "\n",
    "                            # calculating the sum of frequencies for each document \n",
    "                            if parts[0] in dl:\n",
    "                                                                                                           \n",
    "                                    dl[parts[0]] += int(parts[2])\n",
    "                            else:\n",
    "\n",
    "                                    dl[parts[0]] = int(parts[2])           \n",
    "                case 'Porter':\n",
    "                    file_path = 'inverse_reg_porter.txt'\n",
    "                    query_terms = list(set([Porter.stem(term) for term in ExpReg.tokenize(query) if term.lower() not in StopWords]))\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "\n",
    "                            if parts[1] in  query_terms:\n",
    "                                # calculate the sum of frequncies of the term  \"parts[1]\" in each document \"parts[2]\"\n",
    "                                if parts[1] not in freq[int(parts[2])-1]:\n",
    "                                                                        \n",
    "                                    freq[int(parts[2])-1][parts[1]] = int(parts[3])\n",
    "                                \n",
    "\n",
    "                                # calculating the number of documents in which term parts[1] appear\n",
    "                                if parts[1]  in n:\n",
    "                                                                        \n",
    "                                    n[parts[1]] +=  1\n",
    "                                \n",
    "                                else:\n",
    "                                    n[parts[1]] =  1\n",
    "                                     \n",
    "                       \n",
    "                    print(n)           \n",
    "                    file_path = 'descripteur_reg_porter.txt'\n",
    "                    \n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                         \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "                            \n",
    "                            avdl += int(parts[2])\n",
    "                            \n",
    "\n",
    "                            # calculating the sum of frequencies for each document \n",
    "                            if parts[0] in dl:\n",
    "                                                                                                           \n",
    "                                    dl[parts[0]] += int(parts[2])\n",
    "                            else:\n",
    "\n",
    "                                    dl[parts[0]] = int(parts[2])\n",
    "                    print(dl)\n",
    "                                  \n",
    "                    \n",
    "                case 'Lancaster':\n",
    "                    file_path = 'inverse_reg_lancaster.txt'\n",
    "                    query_terms = list(set([Lancaster.stem(term) for term in ExpReg.tokenize(query) if term.lower() not in StopWords]))\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "                            if parts[1] in  query_terms:\n",
    "                                # calculate the sum of frequncies of the term  \"parts[1]\" in each document \"parts[2]\"\n",
    "                                if parts[1] not in freq[int(parts[2])-1]:\n",
    "                                                                        \n",
    "                                    freq[int(parts[2])-1][parts[1]] = int(parts[3])\n",
    "                                \n",
    "\n",
    "                                # calculating the number of documents in which term parts[1] appear\n",
    "                                if parts[1]  in n:\n",
    "                                                                        \n",
    "                                    n[parts[1]] +=  1\n",
    "                                \n",
    "                                else:\n",
    "                                    n[parts[1]] =  1\n",
    "                                \n",
    "                    file_path = 'descripteur_reg_lancaster.txt'\n",
    "                    \n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                         \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "                            \n",
    "                            avdl += int(parts[2])\n",
    "                            \n",
    "\n",
    "                            # calculating the sum of frequencies for each document \n",
    "                            if parts[0] in dl:\n",
    "                                                                                                           \n",
    "                                    dl[parts[0]] += int(parts[2])\n",
    "                            else:\n",
    "\n",
    "                                    dl[parts[0]] = int(parts[2])           \n",
    "\n",
    "    avdl /= N\n",
    "    for i  in range(len(freq)):\n",
    "                         sum = 0.0\n",
    "                         for term, f in freq[i].items():\n",
    "                              print(f'{term}:{f}')\n",
    "                              sum += ( f / ( K * ( (1-B) + B * ( dl[str(i+1)] / avdl) ) + f ) ) * math.log10( ( len(freq) - n[term] + 0.5) /(n[term] + 0.5))                         \n",
    "                         bm25_dict[str(i+1)] = sum\n",
    "                         \n",
    "    bm25_dict = dict(sorted(bm25_dict.items(),key=lambda item:item[1],reverse=True))\n",
    "    return bm25_dict\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'retriev': 2, 'solut': 1, 'llm-base': 3, 'inform': 4}\n",
      "{'1': 113, '2': 138, '3': 134, '4': 131, '5': 151, '6': 121}\n",
      "retriev:1\n",
      "solut:3\n",
      "llm-base:2\n",
      "llm-base:2\n",
      "inform:1\n",
      "retriev:5\n",
      "inform:2\n",
      "inform:3\n",
      "llm-base:1\n",
      "inform:1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'2': 0.3714669065877922,\n",
       " '1': 0.10895313961890599,\n",
       " '4': 0.05046066014145881,\n",
       " '3': -0.10118447385382555,\n",
       " '6': -0.10585698409230639,\n",
       " '5': -0.16404055781562094}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dict = bm25('LLM-based solutions for information retrieval','Porter','Reg',1.50,0.75)\n",
    "my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid queries:\n",
      "Terme: True\n",
      "Terme AND Terme: True\n",
      "Terme OR Terme: True\n",
      "Terme AND Terme OR Terme: True\n",
      "NOT Terme: True\n",
      "NOT Terme AND Terme: True\n",
      "NOT Terme OR NOT Terme: True\n",
      "Terme AND NOT Terme: True\n",
      "NOT Terme AND Terme OR NOT Terme: True\n",
      "\n",
      "Invalid queries:\n",
      "AND: False\n",
      "OR: False\n",
      "Terme Terme: False\n",
      "AND Terme: False\n",
      "Terme OR: False\n",
      "AND OR Terme: False\n",
      "Terme AND OR Terme: False\n",
      "Terme AND Terme AND: False\n",
      "NOT: False\n",
      "NOT NOT Terme: False\n",
      "NOT AND Terme: False\n",
      "Terme AND NOT: False\n",
      "Terme AND TERME NOT: False\n"
     ]
    }
   ],
   "source": [
    "def validate_logic_query(query):\n",
    "    # Tokenize the query\n",
    "    tokens = ExpReg.tokenize(query) \n",
    "    operators = {'AND', 'OR', 'NOT'}\n",
    "    valid = True\n",
    "    \n",
    "    #  empty query or single invalid operator or query that ends with an operator \n",
    "    if not tokens or tokens[0] in {'AND', 'OR'} or tokens[-1] in operators:\n",
    "        return False\n",
    "    \n",
    "  \n",
    "    expect_term = True  # to expect a term or not \n",
    "    previous_token = None\n",
    "\n",
    "    for token in tokens:\n",
    "        if token in operators:\n",
    "            # to Check invalid operator sequences\n",
    "            if previous_token in operators:\n",
    "                if token != 'NOT' or previous_token == 'NOT':\n",
    "                    return False\n",
    "            \n",
    "            if token in {'AND', 'OR'}:\n",
    "                # AND/OR must follow a term\n",
    "                if expect_term:\n",
    "                    return False\n",
    "                expect_term = True  # After AND/OR, we expect a term\n",
    "            elif token == 'NOT':\n",
    "                # NOT must precede a term \n",
    "                expect_term = True\n",
    "\n",
    "        else:\n",
    "            # Token is a term\n",
    "            if not expect_term:\n",
    "                return False\n",
    "            expect_term = False  # After a term, we expect an operator\n",
    "        \n",
    "        previous_token = token\n",
    "\n",
    "    \n",
    "    return valid\n",
    "\n",
    "\n",
    "# Test cases\n",
    "valid_queries = [\n",
    "    \"Terme\",\n",
    "    \"Terme AND Terme\",\n",
    "    \"Terme OR Terme\",\n",
    "    \"Terme AND Terme OR Terme\",\n",
    "    \"NOT Terme\",\n",
    "    \"NOT Terme AND Terme\",\n",
    "    \"NOT Terme OR NOT Terme\",\n",
    "    \"Terme AND NOT Terme\",\n",
    "    \"NOT Terme AND Terme OR NOT Terme\"\n",
    "]\n",
    "\n",
    "invalid_queries = [\n",
    "    \"AND\",\n",
    "    \"OR\",\n",
    "    \"Terme Terme\",\n",
    "    \"AND Terme\",\n",
    "    \"Terme OR\",\n",
    "    \"AND OR Terme\",\n",
    "    \"Terme AND OR Terme\",\n",
    "    \"Terme AND Terme AND\",\n",
    "    \"NOT\",\n",
    "    \"NOT NOT Terme\",\n",
    "    \"NOT AND Terme\",\n",
    "    \"Terme AND NOT\",\n",
    "    \"Terme AND TERME NOT\"\n",
    "]\n",
    "\n",
    "print(\"Valid queries:\")\n",
    "for q in valid_queries:\n",
    "    print(f\"{q}: {validate_logic_query(q)}\")\n",
    "\n",
    "print(\"\\nInvalid queries:\")\n",
    "for q in invalid_queries:\n",
    "    print(f\"{q}: {validate_logic_query(q)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy.logic.boolalg import Or, And, Not\n",
    "from sympy import symbols, simplify_logic  \n",
    "\n",
    "def boolean_model(query,stemming,preprocessing):\n",
    "   \n",
    "    boolean_dict = {}\n",
    "    terms_dictionnaries = []\n",
    "    N = 6\n",
    " \n",
    " \n",
    " \n",
    "    \n",
    "    \n",
    "    file_path = \"\"\n",
    "    if preprocessing == \"Split\":\n",
    "            match stemming:\n",
    "                case 'No Stemming':\n",
    "                   \n",
    "                    if not validate_logic_query(query):\n",
    "                        return None\n",
    "\n",
    "                    \n",
    "                    file_path = 'inverse_split.txt'\n",
    "                    query_terms = list([term for term in query.split() if term.lower() not in StopWords])                     \n",
    "                    print('Query terms:', query_terms)\n",
    "\n",
    "                    # Extracting query terms without operators\n",
    "                    query_terms_without_operators = [term for term in query_terms if term not in ['and', 'or', 'not']]\n",
    "                    print('Terms without operators:', query_terms_without_operators)\n",
    "\n",
    "                    # Initialize variables\n",
    "                    operators = ['and', 'or', 'not']\n",
    "                    terms_dictionaries = [[] for _ in range(N)] \n",
    "\n",
    "                    # looping through the terms of the query to calculate for each document the terms of the query that it contains\n",
    "                    for term in query_terms_without_operators:\n",
    "                         with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                              lines = file.readlines()\n",
    "                              for line in lines:\n",
    "                                   parts = line.split()\n",
    "                    \n",
    "                                   if parts[1] == term:\n",
    "                                        doc_id = int(parts[2]) - 1  \n",
    "                                        terms_dictionaries[doc_id].append(term)\n",
    "\n",
    "                    print('Terms dictionaries:', terms_dictionaries)\n",
    "\n",
    "                    for i in range(N):\n",
    "                        query_modified = query\n",
    "                        query_terms_without_stemming = [term for term in query.split() if term.lower() not in operators]\n",
    "                        \n",
    "                        for term in query_terms_without_stemming:\n",
    "                              #print(term)\n",
    "                              #print('\\n')\n",
    "                              # Replacing terms with 'True' or 'False' based on their presence in the document\n",
    "                            \n",
    "                              if term in terms_dictionaries[i]:\n",
    "                                   query_modified = query_modified.replace(term, 'True')\n",
    "                              else:\n",
    "                                   query_modified = query_modified.replace(term, 'False')\n",
    "    \n",
    "                        query_modified = query_modified.replace('OR', 'or')\n",
    "                        query_modified = query_modified.replace('AND', 'and')\n",
    "                        query_modified = query_modified.replace('NOT', 'not')\n",
    "\n",
    "                        print(f\"Document {i + 1} Final Query for Simplification: {query_modified}\")\n",
    "\n",
    "                        \n",
    "\n",
    "                        try:\n",
    "                            boolean_dict[i] = eval(query_modified)\n",
    "                        except Exception as e:\n",
    "                           print(f\"Error evaluating query for document {i + 1}: {e}\")\n",
    "                           boolean_dict[i] = False  \n",
    "\n",
    "                    print('Boolean Dictionary:', boolean_dict)                                \n",
    "                         \n",
    "                case 'Porter':\n",
    "                    if not validate_logic_query(query):\n",
    "                        return None\n",
    "\n",
    "                    \n",
    "                    file_path = 'inverse_split_porter.txt'\n",
    "                    query_terms = list([Porter.stem(term) for term in query.split() if term.lower() not in StopWords])                     \n",
    "                    print('Query terms:', query_terms)\n",
    "\n",
    "                    # Extracting query terms without operators\n",
    "                    query_terms_without_operators = [term for term in query_terms if term not in ['and', 'or', 'not']]\n",
    "                    print('Terms without operators:', query_terms_without_operators)\n",
    "\n",
    "                    # Initialize variables\n",
    "                    operators = ['and', 'or', 'not']\n",
    "                    terms_dictionaries = [[] for _ in range(N)] \n",
    "\n",
    "                    # looping through the terms of the query to calculate for each document the terms of the query that it contains\n",
    "                    for term in query_terms_without_operators:\n",
    "                         with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                              lines = file.readlines()\n",
    "                              for line in lines:\n",
    "                                   parts = line.split()\n",
    "                    \n",
    "                                   if parts[1] == term:\n",
    "                                        doc_id = int(parts[2]) - 1  \n",
    "                                        terms_dictionaries[doc_id].append(term)\n",
    "\n",
    "                    print('Terms dictionaries:', terms_dictionaries)\n",
    "\n",
    "                    for i in range(N):\n",
    "                        query_modified = query\n",
    "                        query_terms_without_stemming = [term for term in query.split() if term.lower() not in operators]\n",
    "                        \n",
    "                        for term in query_terms_without_stemming:\n",
    "                              #print(term)\n",
    "                              #print('\\n')\n",
    "                              # Replacing terms with 'True' or 'False' based on their presence in the document\n",
    "                            \n",
    "                              if Porter.stem(term) in terms_dictionaries[i]:\n",
    "                                   query_modified = query_modified.replace(term, 'True')\n",
    "                              else:\n",
    "                                   query_modified = query_modified.replace(term, 'False')\n",
    "    \n",
    "                        query_modified = query_modified.replace('OR', 'or')\n",
    "                        query_modified = query_modified.replace('AND', 'and')\n",
    "                        query_modified = query_modified.replace('NOT', 'not')\n",
    "\n",
    "                        print(f\"Document {i + 1} Final Query for Simplification: {query_modified}\")\n",
    "\n",
    "                        \n",
    "\n",
    "                        try:\n",
    "                            boolean_dict[i] = eval(query_modified)\n",
    "                        except Exception as e:\n",
    "                           print(f\"Error evaluating query for document {i + 1}: {e}\")\n",
    "                           boolean_dict[i] = False  \n",
    "\n",
    "                    print('Boolean Dictionary:', boolean_dict)            \n",
    "                                \n",
    "                case 'Lancaster':\n",
    "                    if not validate_logic_query(query):\n",
    "                        return None\n",
    "\n",
    "                    \n",
    "                    file_path = 'inverse_split_lancaster.txt'\n",
    "                    query_terms = list([Lancaster.stem(term) for term in query.split() if term.lower() not in StopWords])                     \n",
    "                    print('Query terms:', query_terms)\n",
    "\n",
    "                    # Extracting query terms without operators\n",
    "                    query_terms_without_operators = [term for term in query_terms if term not in ['and', 'or', 'not']]\n",
    "                    print('Terms without operators:', query_terms_without_operators)\n",
    "\n",
    "                    # Initialize variables\n",
    "                    operators = ['and', 'or', 'not']\n",
    "                    terms_dictionaries = [[] for _ in range(N)] \n",
    "\n",
    "                    # looping through the terms of the query to calculate for each document the terms of the query that it contains\n",
    "                    for term in query_terms_without_operators:\n",
    "                         with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                              lines = file.readlines()\n",
    "                              for line in lines:\n",
    "                                   parts = line.split()\n",
    "                    \n",
    "                                   if parts[1] == term:\n",
    "                                        doc_id = int(parts[2]) - 1  \n",
    "                                        terms_dictionaries[doc_id].append(term)\n",
    "\n",
    "                    print('Terms dictionaries:', terms_dictionaries)\n",
    "\n",
    "                    for i in range(N):\n",
    "                        query_modified = query\n",
    "                        query_terms_without_stemming = [term for term in query.split() if term.lower() not in operators]\n",
    "                        \n",
    "                        for term in query_terms_without_stemming:\n",
    "                              #print(term)\n",
    "                              #print('\\n')\n",
    "                              # Replacing terms with 'True' or 'False' based on their presence in the document\n",
    "                            \n",
    "                              if Lancaster.stem(term) in terms_dictionaries[i]:\n",
    "                                   query_modified = query_modified.replace(term, 'True')\n",
    "                              else:\n",
    "                                   query_modified = query_modified.replace(term, 'False')\n",
    "    \n",
    "                        query_modified = query_modified.replace('OR', 'or')\n",
    "                        query_modified = query_modified.replace('AND', 'and')\n",
    "                        query_modified = query_modified.replace('NOT', 'not')\n",
    "\n",
    "                        print(f\"Document {i + 1} Final Query for Simplification: {query_modified}\")\n",
    "\n",
    "                        \n",
    "\n",
    "                        try:\n",
    "                            boolean_dict[i] = eval(query_modified)\n",
    "                        except Exception as e:\n",
    "                           print(f\"Error evaluating query for document {i + 1}: {e}\")\n",
    "                           boolean_dict[i] = False  \n",
    "\n",
    "                    print('Boolean Dictionary:', boolean_dict)             \n",
    "    else:\n",
    "            match stemming:\n",
    "                case 'No Stemming':\n",
    "                    if not validate_logic_query(query):\n",
    "                        return None\n",
    "\n",
    "                    \n",
    "                    file_path = 'inverse_reg.txt'\n",
    "                    query_terms = list([term for term in ExpReg.tokenize(query) if term.lower() not in StopWords])                     \n",
    "                    print('Query terms:', query_terms)\n",
    "\n",
    "                    # Extracting query terms without operators\n",
    "                    query_terms_without_operators = [term for term in query_terms if term not in ['and', 'or', 'not']]\n",
    "                    print('Terms without operators:', query_terms_without_operators)\n",
    "\n",
    "                    # Initialize variables\n",
    "                    operators = ['and', 'or', 'not']\n",
    "                    terms_dictionaries = [[] for _ in range(N)] \n",
    "\n",
    "                    # looping through the terms of the query to calculate for each document the terms of the query that it contains\n",
    "                    for term in query_terms_without_operators:\n",
    "                         with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                              lines = file.readlines()\n",
    "                              for line in lines:\n",
    "                                   parts = line.split()\n",
    "                    \n",
    "                                   if parts[1] == term:\n",
    "                                        doc_id = int(parts[2]) - 1  \n",
    "                                        terms_dictionaries[doc_id].append(term)\n",
    "\n",
    "                    print('Terms dictionaries:', terms_dictionaries)\n",
    "\n",
    "                    for i in range(N):\n",
    "                        query_modified = query\n",
    "                        query_terms_without_stemming = [term for term in ExpReg.tokenize(query) if term.lower() not in operators]\n",
    "                        \n",
    "                        for term in query_terms_without_stemming:\n",
    "                              #print(term)\n",
    "                              #print('\\n')\n",
    "                              # Replacing terms with 'True' or 'False' based on their presence in the document\n",
    "                            \n",
    "                              if term in terms_dictionaries[i]:\n",
    "                                   query_modified = query_modified.replace(term, 'True')\n",
    "                              else:\n",
    "                                   query_modified = query_modified.replace(term, 'False')\n",
    "    \n",
    "                        query_modified = query_modified.replace('OR', 'or')\n",
    "                        query_modified = query_modified.replace('AND', 'and')\n",
    "                        query_modified = query_modified.replace('NOT', 'not')\n",
    "\n",
    "                        print(f\"Document {i + 1} Final Query for Simplification: {query_modified}\")\n",
    "\n",
    "                        \n",
    "\n",
    "                        try:\n",
    "                            boolean_dict[i] = eval(query_modified)\n",
    "                        except Exception as e:\n",
    "                           print(f\"Error evaluating query for document {i + 1}: {e}\")\n",
    "                           boolean_dict[i] = False  \n",
    "\n",
    "                    print('Boolean Dictionary:', boolean_dict)            \n",
    "                case 'Porter':\n",
    "\n",
    "                    \n",
    "                    if not validate_logic_query(query):\n",
    "                        return None\n",
    "\n",
    "                    \n",
    "                    file_path = 'inverse_reg_porter.txt'\n",
    "                    query_terms = list([Porter.stem(term) for term in ExpReg.tokenize(query) if term.lower() not in StopWords])                     \n",
    "                    print('Query terms:', query_terms)\n",
    "\n",
    "                    # Extracting query terms without operators\n",
    "                    query_terms_without_operators = [term for term in query_terms if term not in ['and', 'or', 'not']]\n",
    "                    print('Terms without operators:', query_terms_without_operators)\n",
    "\n",
    "                    # Initialize variables\n",
    "                    operators = ['and', 'or', 'not']\n",
    "                    terms_dictionaries = [[] for _ in range(N)] \n",
    "\n",
    "                    # looping through the terms of the query to calculate for each document the terms of the query that it contains\n",
    "                    for term in query_terms_without_operators:\n",
    "                         with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                              lines = file.readlines()\n",
    "                              for line in lines:\n",
    "                                   parts = line.split()\n",
    "                    \n",
    "                                   if parts[1] == term:\n",
    "                                        doc_id = int(parts[2]) - 1  \n",
    "                                        terms_dictionaries[doc_id].append(term)\n",
    "\n",
    "                    print('Terms dictionaries:', terms_dictionaries)\n",
    "\n",
    "                    for i in range(N):\n",
    "                        query_modified = query\n",
    "                        query_terms_without_stemming = [term for term in ExpReg.tokenize(query) if term.lower() not in operators]\n",
    "                        \n",
    "                        for term in query_terms_without_stemming:\n",
    "                              #print(term)\n",
    "                              #print('\\n')\n",
    "                              # Replacing terms with 'True' or 'False' based on their presence in the document\n",
    "                            \n",
    "                              if Porter.stem(term) in terms_dictionaries[i]:\n",
    "                                   query_modified = query_modified.replace(term, 'True')\n",
    "                              else:\n",
    "                                   query_modified = query_modified.replace(term, 'False')\n",
    "    \n",
    "                        query_modified = query_modified.replace('OR', 'or')\n",
    "                        query_modified = query_modified.replace('AND', 'and')\n",
    "                        query_modified = query_modified.replace('NOT', 'not')\n",
    "\n",
    "                        print(f\"Document {i + 1} Final Query for Simplification: {query_modified}\")\n",
    "\n",
    "                        \n",
    "\n",
    "                        try:\n",
    "                            boolean_dict[i+1] = eval(query_modified)\n",
    "                        except Exception as e:\n",
    "                           print(f\"Error evaluating query for document {i + 1}: {e}\")\n",
    "                           boolean_dict[i+1] = False  \n",
    "\n",
    "                    print('Boolean Dictionary:', boolean_dict)\n",
    "                    \n",
    "                 \n",
    "                case 'Lancaster':\n",
    "                    if not validate_logic_query(query):\n",
    "                        return None\n",
    "\n",
    "                    \n",
    "                    file_path = 'inverse_reg_lancaster.txt'\n",
    "                    query_terms = list([Lancaster.stem(term) for term in ExpReg.tokenize(query) if term.lower() not in StopWords])                     \n",
    "                    print('Query terms:', query_terms)\n",
    "\n",
    "                    # Extracting query terms without operators\n",
    "                    query_terms_without_operators = [term for term in query_terms if term not in ['and', 'or', 'not']]\n",
    "                    print('Terms without operators:', query_terms_without_operators)\n",
    "\n",
    "                    # Initialize variables\n",
    "                    operators = ['and', 'or', 'not']\n",
    "                    terms_dictionaries = [[] for _ in range(N)] \n",
    "\n",
    "                    # looping through the terms of the query to calculate for each document the terms of the query that it contains\n",
    "                    for term in query_terms_without_operators:\n",
    "                         with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                              lines = file.readlines()\n",
    "                              for line in lines:\n",
    "                                   parts = line.split()\n",
    "                    \n",
    "                                   if parts[1] == term:\n",
    "                                        doc_id = int(parts[2]) - 1  \n",
    "                                        terms_dictionaries[doc_id].append(term)\n",
    "\n",
    "                    print('Terms dictionaries:', terms_dictionaries)\n",
    "\n",
    "                    for i in range(N):\n",
    "                        query_modified = query\n",
    "                        query_terms_without_stemming = [term for term in ExpReg.tokenize(query) if term.lower() not in operators]\n",
    "                        \n",
    "                        for term in query_terms_without_stemming:\n",
    "                              #print(term)\n",
    "                              #print('\\n')\n",
    "                              # Replacing terms with 'True' or 'False' based on their presence in the document\n",
    "                            \n",
    "                              if Lancaster.stem(term) in terms_dictionaries[i]:\n",
    "                                   query_modified = query_modified.replace(term, 'True')\n",
    "                              else:\n",
    "                                   query_modified = query_modified.replace(term, 'False')\n",
    "    \n",
    "                        query_modified = query_modified.replace('OR', 'or')\n",
    "                        query_modified = query_modified.replace('AND', 'and')\n",
    "                        query_modified = query_modified.replace('NOT', 'not')\n",
    "\n",
    "                        print(f\"Document {i + 1} Final Query for Simplification: {query_modified}\")\n",
    "\n",
    "                        \n",
    "\n",
    "                        try:\n",
    "                            boolean_dict[i+1] = eval(query_modified)\n",
    "                        except Exception as e:\n",
    "                           print(f\"Error evaluating query for document {i + 1}: {e}\")\n",
    "                           boolean_dict[i+1] = False  \n",
    "\n",
    "                    print('Boolean Dictionary:', boolean_dict)           \n",
    "\n",
    "    return boolean_dict\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query terms: ['chatgpt', 'gpt-3.5']\n",
      "Terms without operators: ['chatgpt', 'gpt-3.5']\n",
      "Terms dictionaries: [[], ['chatgpt'], [], [], [], ['gpt-3.5']]\n",
      "Document 1 Final Query for Simplification: False or False\n",
      "Document 2 Final Query for Simplification: True or False\n",
      "Document 3 Final Query for Simplification: False or False\n",
      "Document 4 Final Query for Simplification: False or False\n",
      "Document 5 Final Query for Simplification: False or False\n",
      "Document 6 Final Query for Simplification: False or True\n",
      "Boolean Dictionary: {1: False, 2: True, 3: False, 4: False, 5: False, 6: True}\n"
     ]
    }
   ],
   "source": [
    "my_dict = boolean_model('ChatGPT OR GPT-3.5','Porter','Reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adfc2fb251a54716a3e9cd875bbcc59b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Text(value='', description='Query:', layout=Layout(width='65%'), placeholder='Enter your query …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4036737e9a1949f3b7b5bb66f37e0f08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Preprocessing:', layout=Layout(width='45%'), options=('Spl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b5293ef48b94a9eb22919b4665bf87a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', description='Results:', layout=Layout(height='300px', width='100%'), placeholder='Results w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vi 4\n",
      "vi 4\n",
      "vi 4\n",
      "vi 4\n",
      "vi 4\n",
      "vi 3\n"
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "border_color = '#ccc'\n",
    "background_color = '#f9f9f9'\n",
    "container_background_color = '#f0f0f0'\n",
    "\n",
    "# Query input with adjusted width\n",
    "query_input = widgets.Text(\n",
    "    description='Query:',\n",
    "    placeholder='Enter your query here',\n",
    "    layout=widgets.Layout(width='65%')\n",
    ")\n",
    "\n",
    "# Submit button with adjusted width\n",
    "submit_button = widgets.Button(\n",
    "    description='Search',\n",
    "    layout=widgets.Layout(width='15%')\n",
    ")\n",
    "\n",
    "# Combine query input and submit button in a single line with styling\n",
    "query_container = widgets.HBox(\n",
    "    [query_input, submit_button],\n",
    "    layout=widgets.Layout(\n",
    "        border=f'2px solid {border_color}',\n",
    "        border_radius='10px',\n",
    "        padding='10px',\n",
    "        background_color=background_color,\n",
    "        width='80%'  # Adjust container width\n",
    "    )\n",
    ")\n",
    "\n",
    "# Dropdowns for preprocessing and stemming parameters\n",
    "preprocessing_options = ['Split', 'Reg']\n",
    "stemming_options = ['Porter', 'Lancaster', 'No Stemming']\n",
    "\n",
    "preprocessing_dropdown = widgets.Dropdown(\n",
    "    options=preprocessing_options,\n",
    "    description='Preprocessing:',\n",
    "    layout=widgets.Layout(width='45%')\n",
    ")\n",
    "\n",
    "stemming_dropdown = widgets.Dropdown(\n",
    "    options=stemming_options,\n",
    "    description='Stemming:',\n",
    "    layout=widgets.Layout(width='45%')\n",
    ")\n",
    "\n",
    "# Group preprocessing and stemming dropdowns in a container\n",
    "preprocessing_container = widgets.HBox(\n",
    "    [preprocessing_dropdown, stemming_dropdown],\n",
    "    layout=widgets.Layout(\n",
    "        border=f'2px solid {border_color}',\n",
    "        border_radius='10px',\n",
    "        padding='10px',\n",
    "        background_color=container_background_color,\n",
    "        justify_content='space-between',\n",
    "        width='80%'  # Adjust container width\n",
    "    )\n",
    ")\n",
    "\n",
    "# RadioButtons for file selection with toggle button\n",
    "file_options = ['DOCS per Term', 'Terms per Doc']\n",
    "file_selection = widgets.RadioButtons(\n",
    "    options=file_options,\n",
    "    description='File Type:',\n",
    "    layout=widgets.Layout(width='50%')\n",
    ")\n",
    "\n",
    "toggle_button = widgets.ToggleButton(\n",
    "    value=False,\n",
    "    description='Enable/Disable File Selection',\n",
    "    button_style='', \n",
    "    tooltip='Click to enable/disable the file selection widget',\n",
    "    layout=widgets.Layout(width='45%')\n",
    ")\n",
    "\n",
    "# Combine toggle button and file selection radio buttons\n",
    "file_selection_container = widgets.HBox(\n",
    "    [toggle_button, file_selection],\n",
    "    layout=widgets.Layout(\n",
    "        border=f'2px solid {border_color}',\n",
    "        border_radius='10px',\n",
    "        padding='10px',\n",
    "        background_color=background_color,\n",
    "        justify_content='space-between',\n",
    "        width='80%'  # Adjust container width\n",
    "    )\n",
    ")\n",
    "\n",
    "# Toggle function to enable/disable the file selection\n",
    "def on_toggle_button_change(change):\n",
    "    file_selection.disabled = not change['new']  \n",
    "\n",
    "toggle_button.observe(on_toggle_button_change, names='value')\n",
    "\n",
    "def is_file_selection_enabled():\n",
    "    return not file_selection.disabled\n",
    "\n",
    "# RadioButtons for model selection\n",
    "models_options = ['Scalar Product', 'Cosine Measure', 'Jaccard Measure','BM25','Boolean']\n",
    "model_selection = widgets.RadioButtons(\n",
    "    options=models_options,\n",
    "    description='Vector space model:',\n",
    "    layout=widgets.Layout(width='80%')  # Adjust width to match other elements\n",
    ")\n",
    "\n",
    "k_input = widgets.Text(\n",
    "    description='K:',\n",
    "    placeholder='',\n",
    "    layout=widgets.Layout(width='65%')\n",
    ")\n",
    "\n",
    "b_input = widgets.Text(\n",
    "    description='B:',\n",
    "    placeholder='',\n",
    "    layout=widgets.Layout(width='65%')\n",
    ")\n",
    "\n",
    "params_container = widgets.VBox(\n",
    "    [k_input, b_input],\n",
    "    layout=widgets.Layout(\n",
    "        border=f'2px solid {border_color}',\n",
    "        border_radius='10px',\n",
    "        padding='15px',\n",
    "        background_color=background_color,\n",
    "        width='85%'  # Adjust container width\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# Group dropdowns and model selection in a single container\n",
    "dropdowns_container = widgets.VBox(\n",
    "    [preprocessing_container, file_selection_container, model_selection, params_container],\n",
    "    layout=widgets.Layout(\n",
    "        border=f'2px solid {border_color}',\n",
    "        border_radius='10px',\n",
    "        padding='15px',\n",
    "        background_color=background_color,\n",
    "        width='85%'  # Adjust container width\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Create a text area to display results\n",
    "result_area = widgets.Textarea(\n",
    "    description='Results:',\n",
    "    placeholder='Results will be displayed here',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(\n",
    "        width='100%',\n",
    "        height='300px',\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Function to handle button click\n",
    "def on_submit(b):\n",
    "    query = query_input.value\n",
    "    preprocessing = preprocessing_dropdown.value\n",
    "    stemming = stemming_dropdown.value\n",
    "    selected_file = file_selection.value\n",
    "    model = model_selection.value\n",
    "    result_content = \"\"\n",
    "    if k_input.value == '':\n",
    "        k = 0\n",
    "    else:\n",
    "        k = float(k_input.value)\n",
    "    \n",
    "    if b_input.value == '':\n",
    "        b = 0\n",
    "    else:\n",
    "        b = float(k_input.value)\n",
    "\n",
    "\n",
    "    if is_file_selection_enabled():\n",
    "        file_path = \"\"\n",
    "        if selected_file == \"Terms per Doc\":  # descriptive file\n",
    "            if preprocessing == \"Split\":\n",
    "                match stemming:\n",
    "                    case 'No Stemming':\n",
    "                        file_path = 'descripteur_split.txt'\n",
    "                    case 'Porter':\n",
    "                        file_path = 'descripteur_split_porter.txt'\n",
    "                    case 'Lancaster':\n",
    "                        file_path = 'descripteur_split_lancaster.txt'\n",
    "            else:\n",
    "                match stemming:\n",
    "                    case 'No Stemming':\n",
    "                        file_path = 'descripteur_reg.txt'\n",
    "                    case 'Porter':\n",
    "                        file_path = 'descripteur_reg_porter.txt'\n",
    "                    case 'Lancaster':\n",
    "                        file_path = 'descripteur_reg_lancaster.txt'\n",
    "        else:  # inverse file\n",
    "            if preprocessing == \"Split\":\n",
    "                match stemming:\n",
    "                    case 'No Stemming':\n",
    "                        file_path = 'inverse_split.txt'\n",
    "                    case 'Porter':\n",
    "                        file_path = 'inverse_split_porter.txt'\n",
    "                        query = Porter.stem(query)\n",
    "                    case 'Lancaster':\n",
    "                        file_path = 'inverse_split_lancaster.txt'\n",
    "                        query = Lancaster.stem(query)\n",
    "            else:\n",
    "                match stemming:\n",
    "                    case 'No Stemming':\n",
    "                        file_path = 'inverse_reg.txt'\n",
    "                    case 'Porter':\n",
    "                        file_path = 'inverse_reg_porter.txt'\n",
    "                        query = Porter.stem(query)\n",
    "                        mdict = rsv(query,'Porter','Reg')\n",
    "                        print(mdict)\n",
    "                        for doc,v in mdict.items():\n",
    "                            formatted_line = f\"{doc:<3} {v:<3}\\n\"\n",
    "                            result_content += formatted_line\n",
    "                    case 'Lancaster':\n",
    "                        file_path = 'inverse_reg_lancaster.txt'\n",
    "                        query = Lancaster.stem(query)\n",
    "\n",
    "        # Initialize result content with the correct header and numbered lines\n",
    "        line_counter = 1\n",
    "        if selected_file == \"Terms per Doc\":\n",
    "            result_content = \"N  Ndoc   Term           Freq   Weight  Positions\\n\"\n",
    "        \n",
    "            # Track terms and frequencies for the selected document\n",
    "            term_count = 0\n",
    "        \n",
    "        \n",
    "             # Filter the file content based on query and file type\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                lines = file.readlines()\n",
    "                for line in lines:\n",
    "                    parts = line.split()\n",
    "                    if parts[0] == query:\n",
    "                        # Format and add the numbered line to result_content\n",
    "                        formatted_line = f\"{line_counter:<3} {parts[0]:<5} {parts[1]:<15} {parts[2]:<5} {parts[3]:<10} {parts[4]:<10}\\n\"\n",
    "                        result_content += formatted_line\n",
    "                        line_counter += 1\n",
    "                        term_count += int(parts[2])  # Increment by term frequency\n",
    "                \n",
    "            # Append document vocabulary and size\n",
    "            result_content += f\"-------------------------------------------------------------------\"\n",
    "            result_content += f\"\\n# Doc vocabulary: {line_counter-1}               \"\n",
    "            result_content += f\"# Doc size: {term_count}\\n\"\n",
    "    \n",
    "        else:\n",
    "            result_content = \"N   Term            Ndoc   Freq   Weight\\n\"\n",
    "        \n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                lines = file.readlines()\n",
    "                for line in lines:\n",
    "                    parts = line.split()\n",
    "                    if parts[1] == query:\n",
    "                        formatted_line = f\"{parts[0]:<3} {parts[1]:<15} {parts[2]:<5} {parts[3]:<5} {parts[4]:<10}\\n\"\n",
    "                        result_content += formatted_line\n",
    "                        line_counter += 1\n",
    "\n",
    "  \n",
    "        if line_counter == 1:  # Only header is present\n",
    "            result_content += \"No matching results found.\"\n",
    "    \n",
    "    else:\n",
    "        if model== 'Scalar Product':\n",
    "            my_dict = rsv(query,stemming,preprocessing)\n",
    "            #print('test')\n",
    "\n",
    "        elif model == 'Cosine Measure':\n",
    "            my_dict = cosine(query,stemming,preprocessing)\n",
    "\n",
    "        elif model == 'Jaccard Measure':\n",
    "            my_dict = jaccard(query,stemming,preprocessing)\n",
    "\n",
    "        elif model == 'BM25':\n",
    "            my_dict = bm25(query,stemming,preprocessing,k,b)\n",
    "        \n",
    "        elif model == 'Boolean':\n",
    "            my_dict = boolean_model(query,stemming,preprocessing)\n",
    "\n",
    "        \n",
    "        if model == 'Boolean':\n",
    "\n",
    "            if my_dict ==None:\n",
    "                result_content = 'query not valid '\n",
    "            else:\n",
    "                for doc,v in my_dict.items():\n",
    "                    formatted_line = f\"{doc:<3} {str(v):<3}\\n\"\n",
    "                    result_content += formatted_line\n",
    "\n",
    "        \n",
    "        else:\n",
    "            for doc,v in my_dict.items():\n",
    "                formatted_line = f\"{doc:<3} {v:<3}\\n\"\n",
    "                result_content += formatted_line\n",
    "\n",
    "\n",
    "    result_area.value = result_content\n",
    "\n",
    "submit_button.on_click(on_submit)\n",
    "\n",
    "# Display all widgets\n",
    "display(query_container, dropdowns_container, result_area)\n",
    "#information retrieval\n",
    "#Large language models (LLM)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
