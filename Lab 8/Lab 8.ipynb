{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import nltk \n",
    "import math\n",
    "\n",
    "Porter = nltk.PorterStemmer()\n",
    "Lancaster = nltk.LancasterStemmer() \n",
    "\n",
    "ExpReg = nltk.RegexpTokenizer('(?:[A-Za-z]\\.)+|[A-Za-z]+[\\-@]\\d+(?:\\.\\d+)?|\\d+[A-Za-z]+|\\d+(?:[\\.\\,\\-]\\d+)?%?|\\w+(?:[\\-/]\\w+)*') # \\d : équivalent à [0-9] \n",
    "StopWords = nltk.corpus.stopwords.words('english') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rsv(query,stemming,preprocessing):\n",
    "   \n",
    "    rsv_dict = {}\n",
    "    \n",
    "    \n",
    "    \n",
    "    file_path = \"\"\n",
    "    if preprocessing == \"Split\":\n",
    "            match stemming:\n",
    "                case 'No Stemming':\n",
    "                    file_path = 'inverse_split.txt'\n",
    "                    query_terms = list(set([term for term in query.split() if term.lower() not in StopWords]))\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:              \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "                            if parts[1] in  query_terms:\n",
    "\n",
    "                                if parts[2] in rsv_dict:\n",
    "                                                                        \n",
    "                                    rsv_dict[parts[2]] += float(parts[4])\n",
    "                                else:\n",
    "\n",
    "                                    rsv_dict[parts[2]] = float(parts[4])                       \n",
    "                         \n",
    "                case 'Porter':\n",
    "                    file_path = 'inverse_split_porter.txt'\n",
    "                    query_terms = list(set([Porter.stem(term) for term in query.split() if term.lower() not in StopWords]))\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "                            if parts[1] in  query_terms:\n",
    "\n",
    "                                if parts[2] in rsv_dict:\n",
    "                                                                        \n",
    "                                    rsv_dict[parts[2]] += float(parts[4])\n",
    "                                else:\n",
    "\n",
    "                                    rsv_dict[parts[2]] = float(parts[4])\n",
    "                case 'Lancaster':\n",
    "                    file_path = 'inverse_split_lancaster.txt'\n",
    "                    query_terms = list(set([Lancaster.stem(term) for term in query.split() if term.lower() not in StopWords]))\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                         \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "                            if parts[1] in  query_terms:\n",
    "\n",
    "                                if parts[2] in rsv_dict:\n",
    "                                                                        \n",
    "                                    rsv_dict[parts[2]] += float(parts[4])\n",
    "                                else:\n",
    "\n",
    "                                    rsv_dict[parts[2]] = float(parts[4])\n",
    "    else:\n",
    "            match stemming:\n",
    "                case 'No Stemming':\n",
    "                    file_path = 'inverse_reg.txt'\n",
    "                    query_terms = list(set([term for term in ExpReg.tokenize(query) if term.lower() not in StopWords]))\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "                            if parts[1] in  query_terms:\n",
    "\n",
    "                                if parts[2] in rsv_dict:\n",
    "                                                                        \n",
    "                                    rsv_dict[parts[2]] += float(parts[4])\n",
    "                                else:\n",
    "\n",
    "                                    rsv_dict[parts[2]] = float(parts[4])\n",
    "                  \n",
    "                case 'Porter':\n",
    "                    file_path = 'inverse_reg_porter.txt'\n",
    "                    query_terms = list(set([Porter.stem(term) for term in ExpReg.tokenize(query) if term.lower() not in StopWords]))\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                         \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "\n",
    "                            if parts[1] in  query_terms:# parts[1] is the term\n",
    "\n",
    "                                if parts[2] in rsv_dict: # parts[2] is the document\n",
    "                                                                        \n",
    "                                    rsv_dict[parts[2]] += float(parts[4]) # parts[4] is the wight of the term in the document\n",
    "                                else:\n",
    "\n",
    "                                    rsv_dict[parts[2]] = float(parts[4])\n",
    "                    \n",
    "                   \n",
    "                case 'Lancaster':\n",
    "                    file_path = 'inverse_reg_lancaster.txt'\n",
    "                    query_terms = list(set([Lancaster.stem(term) for term in ExpReg.tokenize(query) if term.lower() not in StopWords]))\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "                            if parts[1] in  query_terms:\n",
    "\n",
    "                                if parts[2] in rsv_dict:\n",
    "                                                                        \n",
    "                                    rsv_dict[parts[2]] += float(parts[4])\n",
    "                                else:\n",
    "                                   \n",
    "                                   rsv_dict[parts[2]] = float(parts[4])\n",
    "\n",
    "    rsv_dict = dict(sorted(rsv_dict.items(),key=lambda item:item[1],reverse=True))\n",
    "    return rsv_dict\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'5': 0.6434526764861874,\n",
       " '2': 0.47914011693478853,\n",
       " '3': 0.4618931647855281,\n",
       " '6': 0.3217263382430937,\n",
       " '4': 0.24793200279139374,\n",
       " '1': 0.18061799739838874}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dict = rsv('Large language models (LLM)','Porter','Reg')\n",
    "my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cosine(query,stemming,preprocessing):\n",
    "   \n",
    "    som_wi = {}\n",
    "    som_wi_carre = {}\n",
    "    cosine_dict = {}\n",
    "    som_vi_squred =  0\n",
    "    \n",
    "    \n",
    "    \n",
    "    file_path = \"\"\n",
    "    if preprocessing == \"Split\":\n",
    "            match stemming:\n",
    "                case 'No Stemming':\n",
    "                    file_path = 'inverse_split.txt'\n",
    "                    query_terms = list(set([term for term in query.split() if term.lower() not in StopWords]))\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:              \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "                            if parts[1] in  query_terms:\n",
    "\n",
    "                                if parts[2] in som_wi:\n",
    "                                                                        \n",
    "                                    som_wi[parts[2]] += float(parts[4])\n",
    "                                else:\n",
    "\n",
    "                                    som_wi[parts[2]] = float(parts[4])\n",
    "                    \n",
    "                    for term in query_terms:\n",
    "                        \n",
    "                        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            \n",
    "                         \n",
    "                            lines = file.readlines()\n",
    "                            for line in lines:\n",
    "                                parts = line.split()\n",
    "\n",
    "                                if parts[1] == term:\n",
    "                                    som_vi_squred += 1\n",
    "                                    break\n",
    "                                \n",
    "                    file_path = 'descripteur_split.txt'\n",
    "                    \n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                         \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "\n",
    "                             # clculate the som of wi squared\n",
    "                            if parts[0] in som_wi_carre:\n",
    "                                                                                                           \n",
    "                                    som_wi_carre[parts[0]] += (float(parts[3]) * float(parts[3]))\n",
    "                            else:\n",
    "\n",
    "                                    som_wi_carre[parts[0]] = (float(parts[3]) * float(parts[3]))                                \n",
    "                         \n",
    "                case 'Porter':\n",
    "                    file_path = 'inverse_split_porter.txt'\n",
    "                    query_terms = list(set([Porter.stem(term) for term in query.split() if term.lower() not in StopWords]))\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "                            if parts[1] in  query_terms:\n",
    "\n",
    "                                if parts[2] in som_wi:\n",
    "                                                                        \n",
    "                                    som_wi[parts[2]] += float(parts[4])\n",
    "                                else:\n",
    "\n",
    "                                    som_wi[parts[2]] = float(parts[4])\n",
    "                    \n",
    "                    for term in query_terms:\n",
    "                        \n",
    "                        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            \n",
    "                         \n",
    "                            lines = file.readlines()\n",
    "                            for line in lines:\n",
    "                                parts = line.split()\n",
    "\n",
    "                                if parts[1] == term:\n",
    "                                    som_vi_squred += 1\n",
    "                                    break\n",
    "                                \n",
    "                    file_path = 'descripteur_split_porter.txt'\n",
    "                    \n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                         \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "\n",
    "                             # clculate the som of wi squared\n",
    "                            if parts[0] in som_wi_carre:\n",
    "                                                                                                           \n",
    "                                    som_wi_carre[parts[0]] += (float(parts[3]) * float(parts[3]))\n",
    "                            else:\n",
    "\n",
    "                                    som_wi_carre[parts[0]] = (float(parts[3]) * float(parts[3]))            \n",
    "                case 'Lancaster':\n",
    "                    file_path = 'inverse_split_lancaster.txt'\n",
    "                    query_terms = list(set([Lancaster.stem(term) for term in query.split() if term.lower() not in StopWords]))\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                         \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "                            if parts[1] in  query_terms:\n",
    "\n",
    "                                if parts[2] in som_wi:\n",
    "                                                                        \n",
    "                                    som_wi[parts[2]] += float(parts[4])\n",
    "                                else:\n",
    "\n",
    "                                    som_wi[parts[2]] = float(parts[4])\n",
    "                    \n",
    "                    for term in query_terms:\n",
    "                        \n",
    "                        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            \n",
    "                         \n",
    "                            lines = file.readlines()\n",
    "                            for line in lines:\n",
    "                                parts = line.split()\n",
    "\n",
    "                                if parts[1] == term:\n",
    "                                    som_vi_squred += 1\n",
    "                                    break\n",
    "\n",
    "                    \n",
    "                    file_path = 'descripteur_split_lancaster.txt'\n",
    "                    \n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                         \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "\n",
    "                             # clculate the som of wi squared\n",
    "                            if parts[0] in som_wi_carre:\n",
    "                                                                                                           \n",
    "                                    som_wi_carre[parts[0]] += (float(parts[3]) * float(parts[3]))\n",
    "                            else:\n",
    "\n",
    "                                    som_wi_carre[parts[0]] = (float(parts[3]) * float(parts[3]))            \n",
    "                             \n",
    "    else:\n",
    "            match stemming:\n",
    "                case 'No Stemming':\n",
    "                    file_path = 'inverse_reg.txt'\n",
    "                    query_terms = list(set([term for term in ExpReg.tokenize(query) if term.lower() not in StopWords]))\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "                            if parts[1] in  query_terms:\n",
    "\n",
    "                                if parts[2] in som_wi:\n",
    "                                                                        \n",
    "                                    som_wi[parts[2]] += float(parts[4])\n",
    "                                else:\n",
    "\n",
    "                                    som_wi[parts[2]] = float(parts[4])\n",
    "                    \n",
    "                    for term in query_terms:\n",
    "                        \n",
    "                        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            \n",
    "                         \n",
    "                            lines = file.readlines()\n",
    "                            for line in lines:\n",
    "                                parts = line.split()\n",
    "\n",
    "                                if parts[1] == term:\n",
    "                                    som_vi_squred += 1\n",
    "                                    break\n",
    "                    \n",
    "                    file_path = 'descripteur_reg.txt'\n",
    "                    \n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                         \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "\n",
    "                             # clculate the som of wi squared\n",
    "                            if parts[0] in som_wi_carre:\n",
    "                                                                                                           \n",
    "                                    som_wi_carre[parts[0]] += (float(parts[3]) * float(parts[3]))\n",
    "                            else:\n",
    "\n",
    "                                    som_wi_carre[parts[0]] = (float(parts[3]) * float(parts[3]))            \n",
    "                                \n",
    "                case 'Porter':\n",
    "                    file_path = 'inverse_reg_porter.txt'\n",
    "                    query_terms = list(set([Porter.stem(term) for term in ExpReg.tokenize(query) if term.lower() not in StopWords]))\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                         \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "\n",
    "                            if parts[1] in  query_terms:\n",
    "                                # clculate the som of wi \n",
    "                                if parts[2] in som_wi:\n",
    "                                                                        \n",
    "                                    som_wi[parts[2]] += float(parts[4])\n",
    "                                else:\n",
    "\n",
    "                                    som_wi[parts[2]] = float(parts[4])\n",
    "\n",
    "                                \n",
    "\n",
    "                    for term in query_terms:\n",
    "                        \n",
    "                        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            \n",
    "                         \n",
    "                            lines = file.readlines()\n",
    "                            for line in lines:\n",
    "                                parts = line.split()\n",
    "\n",
    "                                if parts[1] == term:\n",
    "                                    som_vi_squred += 1\n",
    "                                    break\n",
    "            \n",
    "            \n",
    "            \n",
    "                                \n",
    "\n",
    "                                \n",
    "                                \n",
    "                                \n",
    "                    file_path = 'descripteur_reg_porter.txt'\n",
    "                    \n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                         \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "\n",
    "                             # clculate the som of wi squared\n",
    "                            if parts[0] in som_wi_carre:\n",
    "                                                                                                           \n",
    "                                    som_wi_carre[parts[0]] += (float(parts[3]) * float(parts[3]))\n",
    "                            else:\n",
    "\n",
    "                                    som_wi_carre[parts[0]] = (float(parts[3]) * float(parts[3]))\n",
    "                    \n",
    "                    \n",
    "                case 'Lancaster':\n",
    "                    file_path = 'inverse_reg_lancaster.txt'\n",
    "                    query_terms = list(set([Lancaster.stem(term) for term in ExpReg.tokenize(query) if term.lower() not in StopWords]))\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "                            if parts[1] in  query_terms:\n",
    "\n",
    "                                if parts[2] in som_wi:\n",
    "                                                                        \n",
    "                                    som_wi[parts[2]] += float(parts[4])\n",
    "                                else:\n",
    "\n",
    "                                    som_wi[parts[2]] = float(parts[4])\n",
    "                    \n",
    "                    for term in query_terms:\n",
    "                        \n",
    "                        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            \n",
    "                         \n",
    "                            lines = file.readlines()\n",
    "                            for line in lines:\n",
    "                                parts = line.split()\n",
    "\n",
    "                                if parts[1] == term:\n",
    "                                    som_vi_squred += 1\n",
    "                                    break\n",
    "                                \n",
    "                    file_path = 'descripteur_reg_lancaster.txt'\n",
    "                    \n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                         \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "\n",
    "                             # clculate the som of wi squared\n",
    "                            if parts[0] in som_wi_carre:\n",
    "                                                                                                           \n",
    "                                    som_wi_carre[parts[0]] += (float(parts[3]) * float(parts[3]))\n",
    "                            else:\n",
    "\n",
    "                                    som_wi_carre[parts[0]] = (float(parts[3]) * float(parts[3]))    \n",
    "                           \n",
    "\n",
    "    \n",
    "    print('vi',som_vi_squred)\n",
    "    for doc,v in som_wi.items():\n",
    "        cosine_dict[doc] =  som_wi[doc] / ( math.sqrt(som_vi_squred) * math.sqrt(som_wi_carre[doc]))\n",
    "    \n",
    "    cosine_dict = dict(sorted(cosine_dict.items(),key=lambda item:item[1],reverse=True))\n",
    "    return cosine_dict\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vi 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'2': 0.36059314302940165,\n",
       " '5': 0.14456576390061102,\n",
       " '6': 0.1217244594306369,\n",
       " '1': 0.10828123288903387,\n",
       " '4': 0.07756421329768295,\n",
       " '3': 0.03657404343700965}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dict = cosine('Ranking documnets using LLMs','Porter','Reg')\n",
    "my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def jaccard(query,stemming,preprocessing):\n",
    "   \n",
    "    som_wi = {}\n",
    "    som_wi_carre = {}\n",
    "    jaccard_dict = {}\n",
    "    som_vi_squred = 0\n",
    "    \n",
    "    \n",
    "    \n",
    "    file_path = \"\"\n",
    "    if preprocessing == \"Split\":\n",
    "            match stemming:\n",
    "                case 'No Stemming':\n",
    "                    file_path = 'inverse_split.txt'\n",
    "                    query_terms = list(set([term for term in query.split() if term.lower() not in StopWords]))\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:              \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "                            if parts[1] in  query_terms:\n",
    "\n",
    "                                if parts[2] in som_wi:\n",
    "                                                                        \n",
    "                                    som_wi[parts[2]] += float(parts[4])\n",
    "                                else:\n",
    "\n",
    "                                    som_wi[parts[2]] = float(parts[4])\n",
    "\n",
    "                    for term in query_terms:\n",
    "                        \n",
    "                        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            \n",
    "                         \n",
    "                            lines = file.readlines()\n",
    "                            for line in lines:\n",
    "                                parts = line.split()\n",
    "\n",
    "                                if parts[1] == term:\n",
    "                                    som_vi_squred += 1\n",
    "                                    break\n",
    "                                \n",
    "                    file_path = 'descripteur_split.txt'\n",
    "                    \n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                         \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "\n",
    "                             # clculate the som of wi squared\n",
    "                            if parts[0] in som_wi_carre:\n",
    "                                                                                                           \n",
    "                                    som_wi_carre[parts[0]] += (float(parts[3]) * float(parts[3]))\n",
    "                            else:\n",
    "\n",
    "                                    som_wi_carre[parts[0]] = (float(parts[3]) * float(parts[3]))                                \n",
    "                         \n",
    "                case 'Porter':\n",
    "                    file_path = 'inverse_split_porter.txt'\n",
    "                    query_terms = list(set([Porter.stem(term) for term in query.split() if term.lower() not in StopWords]))\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "                            if parts[1] in  query_terms:\n",
    "\n",
    "                                if parts[2] in som_wi:\n",
    "                                                                        \n",
    "                                    som_wi[parts[2]] += float(parts[4])\n",
    "                                else:\n",
    "\n",
    "                                    som_wi[parts[2]] = float(parts[4])\n",
    "                    \n",
    "                    for term in query_terms:\n",
    "                        \n",
    "                        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            \n",
    "                         \n",
    "                            lines = file.readlines()\n",
    "                            for line in lines:\n",
    "                                parts = line.split()\n",
    "\n",
    "                                if parts[1] == term:\n",
    "                                    som_vi_squred += 1\n",
    "                                    break\n",
    "                    \n",
    "                    file_path = 'descripteur_split_porter.txt'\n",
    "                    \n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                         \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "\n",
    "                             # clculate the som of wi squared\n",
    "                            if parts[0] in som_wi_carre:\n",
    "                                                                                                           \n",
    "                                    som_wi_carre[parts[0]] += (float(parts[3]) * float(parts[3]))\n",
    "                            else:\n",
    "\n",
    "                                    som_wi_carre[parts[0]] = (float(parts[3]) * float(parts[3]))            \n",
    "                                \n",
    "                case 'Lancaster':\n",
    "                    file_path = 'inverse_split_lancaster.txt'\n",
    "                    query_terms = list(set([Lancaster.stem(term) for term in query.split() if term.lower() not in StopWords]))\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                         \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "                            if parts[1] in  query_terms:\n",
    "\n",
    "                                if parts[2] in som_wi:\n",
    "                                                                        \n",
    "                                    som_wi[parts[2]] += float(parts[4])\n",
    "                                else:\n",
    "\n",
    "                                    som_wi[parts[2]] = float(parts[4])\n",
    "                    \n",
    "                    for term in query_terms:\n",
    "                        \n",
    "                        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            \n",
    "                         \n",
    "                            lines = file.readlines()\n",
    "                            for line in lines:\n",
    "                                parts = line.split()\n",
    "\n",
    "                                if parts[1] == term:\n",
    "                                    som_vi_squred += 1\n",
    "                                    break\n",
    "                                \n",
    "                    file_path = 'descripteur_split_lancaster.txt'\n",
    "                    \n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                         \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "\n",
    "                             # clculate the som of wi squared\n",
    "                            if parts[0] in som_wi_carre:\n",
    "                                                                                                           \n",
    "                                    som_wi_carre[parts[0]] += (float(parts[3]) * float(parts[3]))\n",
    "                            else:\n",
    "\n",
    "                                    som_wi_carre[parts[0]] = (float(parts[3]) * float(parts[3]))            \n",
    "    else:\n",
    "            match stemming:\n",
    "                case 'No Stemming':\n",
    "                    file_path = 'inverse_reg.txt'\n",
    "                    query_terms = list(set([term for term in ExpReg.tokenize(query) if term.lower() not in StopWords]))\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "                            if parts[1] in  query_terms:\n",
    "\n",
    "                                if parts[2] in som_wi:\n",
    "                                                                        \n",
    "                                    som_wi[parts[2]] += float(parts[4])\n",
    "                                else:\n",
    "\n",
    "                                    som_wi[parts[2]] = float(parts[4])\n",
    "                    for term in query_terms:\n",
    "                        \n",
    "                        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            \n",
    "                         \n",
    "                            lines = file.readlines()\n",
    "                            for line in lines:\n",
    "                                parts = line.split()\n",
    "\n",
    "                                if parts[1] == term:\n",
    "                                    som_vi_squred += 1\n",
    "                                    break\n",
    "                                \n",
    "                    file_path = 'descripteur_reg.txt'\n",
    "                    \n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                         \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "\n",
    "                             # clculate the som of wi squared\n",
    "                            if parts[0] in som_wi_carre:\n",
    "                                                                                                           \n",
    "                                    som_wi_carre[parts[0]] += (float(parts[3]) * float(parts[3]))\n",
    "                            else:\n",
    "\n",
    "                                    som_wi_carre[parts[0]] = (float(parts[3]) * float(parts[3]))            \n",
    "                case 'Porter':\n",
    "                    file_path = 'inverse_reg_porter.txt'\n",
    "                    query_terms = list(set([Porter.stem(term) for term in ExpReg.tokenize(query) if term.lower() not in StopWords]))\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                         \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "\n",
    "                            if parts[1] in  query_terms:\n",
    "                                # clculate the som of wi \n",
    "                                if parts[2] in som_wi:\n",
    "                                                                        \n",
    "                                    som_wi[parts[2]] += float(parts[4])\n",
    "                                else:\n",
    "\n",
    "                                    som_wi[parts[2]] = float(parts[4])\n",
    "                    \n",
    "                    for term in query_terms:\n",
    "                        \n",
    "                        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            \n",
    "                         \n",
    "                            lines = file.readlines()\n",
    "                            for line in lines:\n",
    "                                parts = line.split()\n",
    "\n",
    "                                if parts[1] == term:\n",
    "                                    som_vi_squred += 1\n",
    "                                    break\n",
    "                                \n",
    "                    file_path = 'descripteur_reg_porter.txt'\n",
    "                    \n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                         \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "\n",
    "                            \n",
    "\n",
    "                             # clculate the som of wi squared\n",
    "                            if parts[0] in som_wi_carre:\n",
    "                                                                                                           \n",
    "                                    som_wi_carre[parts[0]] += (float(parts[3]) * float(parts[3]))\n",
    "                            else:\n",
    "\n",
    "                                    som_wi_carre[parts[0]] = (float(parts[3]) * float(parts[3]))\n",
    "\n",
    "                                    \n",
    "                    \n",
    "                    \n",
    "                case 'Lancaster':\n",
    "                    file_path = 'inverse_reg_lancaster.txt'\n",
    "                    query_terms = list(set([Lancaster.stem(term) for term in ExpReg.tokenize(query) if term.lower() not in StopWords]))\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "                            if parts[1] in  query_terms:\n",
    "\n",
    "                                if parts[2] in som_wi:\n",
    "                                                                        \n",
    "                                    som_wi[parts[2]] += float(parts[4])\n",
    "                                else:\n",
    "\n",
    "                                    som_wi[parts[2]] = float(parts[4])\n",
    "\n",
    "                    for term in query_terms:\n",
    "                        \n",
    "                        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            \n",
    "                         \n",
    "                            lines = file.readlines()\n",
    "                            for line in lines:\n",
    "                                parts = line.split()\n",
    "\n",
    "                                if parts[1] == term:\n",
    "                                    som_vi_squred += 1\n",
    "                                    break\n",
    "                                \n",
    "                    file_path = 'descripteur_reg_lancaster.txt'\n",
    "                    \n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                         \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "\n",
    "                             # clculate the som of wi squared\n",
    "                            if parts[0] in som_wi_carre:\n",
    "                                                                                                           \n",
    "                                    som_wi_carre[parts[0]] += (float(parts[3]) * float(parts[3]))\n",
    "                            else:\n",
    "\n",
    "                                    som_wi_carre[parts[0]] = (float(parts[3]) * float(parts[3]))            \n",
    "\n",
    "   \n",
    "    \n",
    "    for doc,v in som_wi.items():\n",
    "        jaccard_dict[doc] =  som_wi[doc] / ( som_vi_squred + som_wi_carre[doc] - som_wi[doc])\n",
    "    \n",
    "    jaccard_dict = dict(sorted(jaccard_dict.items(),key=lambda item:item[1],reverse=True))\n",
    "    return jaccard_dict\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2': 0.21982563356714666,\n",
       " '5': 0.07780405871287085,\n",
       " '6': 0.06347912118488071,\n",
       " '1': 0.05723930538097438,\n",
       " '4': 0.03683976214683828,\n",
       " '3': 0.018058171658908724}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dict = jaccard('Ranking documnets using LLMs','Porter','Reg')\n",
    "my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bm25(query,stemming,preprocessing,K,B):\n",
    "   \n",
    "    dl = {}\n",
    "    avdl = 0\n",
    "    freq = []\n",
    "    n = {}\n",
    "    bm25_dict = {}\n",
    "    N = 6\n",
    "    \n",
    "    #initialization of the frequencies dictionnaries of each document \n",
    "    for i in range(6):\n",
    "         freq.append({})\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    file_path = \"\"\n",
    "    if preprocessing == \"Split\":\n",
    "            match stemming:\n",
    "                case 'No Stemming':\n",
    "                    file_path = 'inverse_split.txt'\n",
    "                    query_terms = list(set([term for term in query.split() if term.lower() not in StopWords]))\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:              \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "                            if parts[1] in  query_terms:\n",
    "                                # calculate the sum of frequncies of the term  \"parts[1]\" in each document \"parts[2]\"\n",
    "                                if parts[1] not in freq[int(parts[2])-1]:\n",
    "                                                                        \n",
    "                                    freq[int(parts[2])-1][parts[1]] = int(parts[3])\n",
    "                                \n",
    "\n",
    "                                # calculating the number of documents in which term parts[1] appear\n",
    "                                if parts[1]  in n:\n",
    "                                                                        \n",
    "                                    n[parts[1]] +=  1\n",
    "                                \n",
    "                                else:\n",
    "                                    n[parts[1]] =  1\n",
    "                                \n",
    "                    file_path = 'descripteur_split.txt'\n",
    "                    \n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                         \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "                            \n",
    "                            avdl += int(parts[2])\n",
    "                            \n",
    "\n",
    "                            # calculating the sum of frequencies for each document \n",
    "                            if parts[0] in dl:\n",
    "                                                                                                           \n",
    "                                    dl[parts[0]] += int(parts[2])\n",
    "                            else:\n",
    "\n",
    "                                    dl[parts[0]] = int(parts[2])                                \n",
    "                         \n",
    "                case 'Porter':\n",
    "                    file_path = 'inverse_split_porter.txt'\n",
    "                    query_terms = list(set([Porter.stem(term) for term in query.split() if term.lower() not in StopWords]))\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "                            if parts[1] in  query_terms:\n",
    "                                # calculate the sum of frequncies of the term  \"parts[1]\" in each document \"parts[2]\"\n",
    "                                if parts[1] not in freq[int(parts[2])-1]:\n",
    "                                                                        \n",
    "                                    freq[int(parts[2])-1][parts[1]] = int(parts[3])\n",
    "                                \n",
    "\n",
    "                                # calculating the number of documents in which term parts[1] appear\n",
    "                                if parts[1]  in n:\n",
    "                                                                        \n",
    "                                    n[parts[1]] +=  1\n",
    "                                \n",
    "                                else:\n",
    "                                    n[parts[1]] =  1\n",
    "                    \n",
    "                    file_path = 'descripteur_split_porter.txt'\n",
    "                    \n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                         \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "                            \n",
    "                            avdl += int(parts[2])\n",
    "                            \n",
    "\n",
    "                            # calculating the sum of frequencies for each document \n",
    "                            if parts[0] in dl:\n",
    "                                                                                                           \n",
    "                                    dl[parts[0]] += int(parts[2])\n",
    "                            else:\n",
    "\n",
    "                                    dl[parts[0]] = int(parts[2])            \n",
    "                                \n",
    "                case 'Lancaster':\n",
    "                    file_path = 'inverse_split_lancaster.txt'\n",
    "                    query_terms = list(set([Lancaster.stem(term) for term in query.split() if term.lower() not in StopWords]))\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                         \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "                            if parts[1] in  query_terms:\n",
    "                                # calculate the sum of frequncies of the term  \"parts[1]\" in each document \"parts[2]\"\n",
    "                                if parts[1] not in freq[int(parts[2])-1]:\n",
    "                                                                        \n",
    "                                    freq[int(parts[2])-1][parts[1]] = int(parts[3])\n",
    "                                \n",
    "\n",
    "                                # calculating the number of documents in which term parts[1] appear\n",
    "                                if parts[1]  in n:\n",
    "                                                                        \n",
    "                                    n[parts[1]] +=  1\n",
    "                                \n",
    "                                else:\n",
    "                                    n[parts[1]] =  1\n",
    "                                \n",
    "                    file_path = 'descripteur_split_lancaster.txt'\n",
    "                    \n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                         \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "                            \n",
    "                            avdl += int(parts[2])\n",
    "                            \n",
    "\n",
    "                            # calculating the sum of frequencies for each document \n",
    "                            if parts[0] in dl:\n",
    "                                                                                                           \n",
    "                                    dl[parts[0]] += int(parts[2])\n",
    "                            else:\n",
    "\n",
    "                                    dl[parts[0]] = int(parts[2])            \n",
    "    else:\n",
    "            match stemming:\n",
    "                case 'No Stemming':\n",
    "                    file_path = 'inverse_reg.txt'\n",
    "                    query_terms = list(set([term for term in ExpReg.tokenize(query) if term.lower() not in StopWords]))\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "                            if parts[1] in  query_terms:\n",
    "                                # calculate the sum of frequncies of the term  \"parts[1]\" in each document \"parts[2]\"\n",
    "                                if parts[1] not in freq[int(parts[2])-1]:\n",
    "                                                                        \n",
    "                                    freq[int(parts[2])-1][parts[1]] = int(parts[3])\n",
    "                                \n",
    "\n",
    "                                # calculating the number of documents in which term parts[1] appear\n",
    "                                if parts[1]  in n:\n",
    "                                                                        \n",
    "                                    n[parts[1]] +=  1\n",
    "                                \n",
    "                                else:\n",
    "                                    n[parts[1]] =  1\n",
    "                                \n",
    "                    file_path = 'descripteur_reg.txt'\n",
    "                    \n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                         \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "                            \n",
    "                            avdl += int(parts[2])\n",
    "                            \n",
    "\n",
    "                            # calculating the sum of frequencies for each document \n",
    "                            if parts[0] in dl:\n",
    "                                                                                                           \n",
    "                                    dl[parts[0]] += int(parts[2])\n",
    "                            else:\n",
    "\n",
    "                                    dl[parts[0]] = int(parts[2])           \n",
    "                case 'Porter':\n",
    "                    file_path = 'inverse_reg_porter.txt'\n",
    "                    query_terms = list(set([Porter.stem(term) for term in ExpReg.tokenize(query) if term.lower() not in StopWords]))\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "\n",
    "                            if parts[1] in  query_terms:\n",
    "                                # calculate the sum of frequncies of the term  \"parts[1]\" in each document \"parts[2]\"\n",
    "                                if parts[1] not in freq[int(parts[2])-1]:\n",
    "                                                                        \n",
    "                                    freq[int(parts[2])-1][parts[1]] = int(parts[3])\n",
    "                                \n",
    "\n",
    "                                # calculating the number of documents in which term parts[1] appear\n",
    "                                if parts[1]  in n:\n",
    "                                                                        \n",
    "                                    n[parts[1]] +=  1\n",
    "                                \n",
    "                                else:\n",
    "                                    n[parts[1]] =  1\n",
    "                                     \n",
    "                       \n",
    "                    print(n)           \n",
    "                    file_path = 'descripteur_reg_porter.txt'\n",
    "                    \n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                         \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "                            \n",
    "                            avdl += int(parts[2])\n",
    "                            \n",
    "\n",
    "                            # calculating the sum of frequencies for each document \n",
    "                            if parts[0] in dl:\n",
    "                                                                                                           \n",
    "                                    dl[parts[0]] += int(parts[2])\n",
    "                            else:\n",
    "\n",
    "                                    dl[parts[0]] = int(parts[2])\n",
    "                    print(dl)\n",
    "                                  \n",
    "                    \n",
    "                case 'Lancaster':\n",
    "                    file_path = 'inverse_reg_lancaster.txt'\n",
    "                    query_terms = list(set([Lancaster.stem(term) for term in ExpReg.tokenize(query) if term.lower() not in StopWords]))\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "                            if parts[1] in  query_terms:\n",
    "                                # calculate the sum of frequncies of the term  \"parts[1]\" in each document \"parts[2]\"\n",
    "                                if parts[1] not in freq[int(parts[2])-1]:\n",
    "                                                                        \n",
    "                                    freq[int(parts[2])-1][parts[1]] = int(parts[3])\n",
    "                                \n",
    "\n",
    "                                # calculating the number of documents in which term parts[1] appear\n",
    "                                if parts[1]  in n:\n",
    "                                                                        \n",
    "                                    n[parts[1]] +=  1\n",
    "                                \n",
    "                                else:\n",
    "                                    n[parts[1]] =  1\n",
    "                                \n",
    "                    file_path = 'descripteur_reg_lancaster.txt'\n",
    "                    \n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                         \n",
    "                        lines = file.readlines()\n",
    "                        for line in lines:\n",
    "                            parts = line.split()\n",
    "                            \n",
    "                            avdl += int(parts[2])\n",
    "                            \n",
    "\n",
    "                            # calculating the sum of frequencies for each document \n",
    "                            if parts[0] in dl:\n",
    "                                                                                                           \n",
    "                                    dl[parts[0]] += int(parts[2])\n",
    "                            else:\n",
    "\n",
    "                                    dl[parts[0]] = int(parts[2])           \n",
    "\n",
    "    avdl /= N\n",
    "    for i  in range(len(freq)):\n",
    "                         sum = 0.0\n",
    "                         for term, f in freq[i].items():\n",
    "                              print(f'{term}:{f}')\n",
    "                              sum += ( f / ( K * ( (1-B) + B * ( dl[str(i+1)] / avdl) ) + f ) ) * math.log10( ( len(freq) - n[term] + 0.5) /(n[term] + 0.5))                         \n",
    "                         bm25_dict[str(i+1)] = sum\n",
    "                         \n",
    "    bm25_dict = dict(sorted(bm25_dict.items(),key=lambda item:item[1],reverse=True))\n",
    "    return bm25_dict\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'retriev': 2, 'solut': 1, 'llm-base': 3, 'inform': 4}\n",
      "{'1': 113, '2': 138, '3': 134, '4': 131, '5': 151, '6': 121}\n",
      "retriev:1\n",
      "solut:3\n",
      "llm-base:2\n",
      "llm-base:2\n",
      "inform:1\n",
      "retriev:5\n",
      "inform:2\n",
      "inform:3\n",
      "llm-base:1\n",
      "inform:1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'2': 0.3714669065877922,\n",
       " '1': 0.10895313961890599,\n",
       " '4': 0.05046066014145881,\n",
       " '3': -0.10118447385382555,\n",
       " '6': -0.10585698409230639,\n",
       " '5': -0.16404055781562094}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dict = bm25('LLM-based solutions for information retrieval','Porter','Reg',1.50,0.75)\n",
    "my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid queries:\n",
      "Terme: True\n",
      "Terme AND Terme: True\n",
      "Terme OR Terme: True\n",
      "Terme AND Terme OR Terme: True\n",
      "NOT Terme: True\n",
      "NOT Terme AND Terme: True\n",
      "NOT Terme OR NOT Terme: True\n",
      "Terme AND NOT Terme: True\n",
      "NOT Terme AND Terme OR NOT Terme: True\n",
      "\n",
      "Invalid queries:\n",
      "AND: False\n",
      "OR: False\n",
      "Terme Terme: False\n",
      "AND Terme: False\n",
      "Terme OR: False\n",
      "AND OR Terme: False\n",
      "Terme AND OR Terme: False\n",
      "Terme AND Terme AND: False\n",
      "NOT: False\n",
      "NOT NOT Terme: False\n",
      "NOT AND Terme: False\n",
      "Terme AND NOT: False\n",
      "Terme AND TERME NOT: False\n"
     ]
    }
   ],
   "source": [
    "def validate_logic_query(query):\n",
    "    # Tokenize the query\n",
    "    tokens = ExpReg.tokenize(query) \n",
    "    operators = {'AND', 'OR', 'NOT'}\n",
    "    valid = True\n",
    "    \n",
    "    #  empty query or single invalid operator or query that ends with an operator \n",
    "    if not tokens or tokens[0] in {'AND', 'OR'} or tokens[-1] in operators:\n",
    "        return False\n",
    "    \n",
    "  \n",
    "    expect_term = True  # to expect a term or not \n",
    "    previous_token = None\n",
    "\n",
    "    for token in tokens:\n",
    "        if token in operators:\n",
    "            # to Check invalid operator sequences\n",
    "            if previous_token in operators:\n",
    "                if token != 'NOT' or previous_token == 'NOT':\n",
    "                    return False\n",
    "            \n",
    "            if token in {'AND', 'OR'}:\n",
    "                # AND/OR must follow a term\n",
    "                if expect_term:\n",
    "                    return False\n",
    "                expect_term = True  # After AND/OR, we expect a term\n",
    "            elif token == 'NOT':\n",
    "                # NOT must precede a term \n",
    "                expect_term = True\n",
    "\n",
    "        else:\n",
    "            # Token is a term\n",
    "            if not expect_term:\n",
    "                return False\n",
    "            expect_term = False  # After a term, we expect an operator\n",
    "        \n",
    "        previous_token = token\n",
    "\n",
    "    \n",
    "    return valid\n",
    "\n",
    "\n",
    "# Test cases\n",
    "valid_queries = [\n",
    "    \"Terme\",\n",
    "    \"Terme AND Terme\",\n",
    "    \"Terme OR Terme\",\n",
    "    \"Terme AND Terme OR Terme\",\n",
    "    \"NOT Terme\",\n",
    "    \"NOT Terme AND Terme\",\n",
    "    \"NOT Terme OR NOT Terme\",\n",
    "    \"Terme AND NOT Terme\",\n",
    "    \"NOT Terme AND Terme OR NOT Terme\"\n",
    "]\n",
    "\n",
    "invalid_queries = [\n",
    "    \"AND\",\n",
    "    \"OR\",\n",
    "    \"Terme Terme\",\n",
    "    \"AND Terme\",\n",
    "    \"Terme OR\",\n",
    "    \"AND OR Terme\",\n",
    "    \"Terme AND OR Terme\",\n",
    "    \"Terme AND Terme AND\",\n",
    "    \"NOT\",\n",
    "    \"NOT NOT Terme\",\n",
    "    \"NOT AND Terme\",\n",
    "    \"Terme AND NOT\",\n",
    "    \"Terme AND TERME NOT\"\n",
    "]\n",
    "\n",
    "print(\"Valid queries:\")\n",
    "for q in valid_queries:\n",
    "    print(f\"{q}: {validate_logic_query(q)}\")\n",
    "\n",
    "print(\"\\nInvalid queries:\")\n",
    "for q in invalid_queries:\n",
    "    print(f\"{q}: {validate_logic_query(q)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy.logic.boolalg import Or, And, Not\n",
    "from sympy import symbols, simplify_logic  \n",
    "\n",
    "def boolean_model(query,stemming,preprocessing):\n",
    "   \n",
    "    boolean_dict = {}\n",
    "    terms_dictionnaries = []\n",
    "    N = 6\n",
    " \n",
    " \n",
    " \n",
    "    \n",
    "    \n",
    "    file_path = \"\"\n",
    "    if preprocessing == \"Split\":\n",
    "            match stemming:\n",
    "                case 'No Stemming':\n",
    "                   \n",
    "                    if not validate_logic_query(query):\n",
    "                        return None\n",
    "\n",
    "                    \n",
    "                    file_path = 'inverse_split.txt'\n",
    "                    query_terms = list([term for term in query.split() if term.lower() not in StopWords])                     \n",
    "                    print('Query terms:', query_terms)\n",
    "\n",
    "                    # Extracting query terms without operators\n",
    "                    query_terms_without_operators = [term for term in query_terms if term not in ['and', 'or', 'not']]\n",
    "                    print('Terms without operators:', query_terms_without_operators)\n",
    "\n",
    "                    # Initialize variables\n",
    "                    operators = ['and', 'or', 'not']\n",
    "                    terms_dictionaries = [[] for _ in range(N)] \n",
    "\n",
    "                    # looping through the terms of the query to calculate for each document the terms of the query that it contains\n",
    "                    for term in query_terms_without_operators:\n",
    "                         with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                              lines = file.readlines()\n",
    "                              for line in lines:\n",
    "                                   parts = line.split()\n",
    "                    \n",
    "                                   if parts[1] == term:\n",
    "                                        doc_id = int(parts[2]) - 1  \n",
    "                                        terms_dictionaries[doc_id].append(term)\n",
    "\n",
    "                    print('Terms dictionaries:', terms_dictionaries)\n",
    "\n",
    "                    for i in range(N):\n",
    "                        query_modified = query\n",
    "                        query_terms_without_stemming = [term for term in query.split() if term.lower() not in operators]\n",
    "                        \n",
    "                        for term in query_terms_without_stemming:\n",
    "                              #print(term)\n",
    "                              #print('\\n')\n",
    "                              # Replacing terms with 'True' or 'False' based on their presence in the document\n",
    "                            \n",
    "                              if term in terms_dictionaries[i]:\n",
    "                                   query_modified = query_modified.replace(term, 'True')\n",
    "                              else:\n",
    "                                   query_modified = query_modified.replace(term, 'False')\n",
    "    \n",
    "                        query_modified = query_modified.replace('OR', 'or')\n",
    "                        query_modified = query_modified.replace('AND', 'and')\n",
    "                        query_modified = query_modified.replace('NOT', 'not')\n",
    "\n",
    "                        print(f\"Document {i + 1} Final Query for Simplification: {query_modified}\")\n",
    "\n",
    "                        \n",
    "\n",
    "                        try:\n",
    "                            boolean_dict[i] = eval(query_modified)\n",
    "                        except Exception as e:\n",
    "                           print(f\"Error evaluating query for document {i + 1}: {e}\")\n",
    "                           boolean_dict[i] = False  \n",
    "\n",
    "                    print('Boolean Dictionary:', boolean_dict)                                \n",
    "                         \n",
    "                case 'Porter':\n",
    "                    if not validate_logic_query(query):\n",
    "                        return None\n",
    "\n",
    "                    \n",
    "                    file_path = 'inverse_split_porter.txt'\n",
    "                    query_terms = list([Porter.stem(term) for term in query.split() if term.lower() not in StopWords])                     \n",
    "                    print('Query terms:', query_terms)\n",
    "\n",
    "                    # Extracting query terms without operators\n",
    "                    query_terms_without_operators = [term for term in query_terms if term not in ['and', 'or', 'not']]\n",
    "                    print('Terms without operators:', query_terms_without_operators)\n",
    "\n",
    "                    # Initialize variables\n",
    "                    operators = ['and', 'or', 'not']\n",
    "                    terms_dictionaries = [[] for _ in range(N)] \n",
    "\n",
    "                    # looping through the terms of the query to calculate for each document the terms of the query that it contains\n",
    "                    for term in query_terms_without_operators:\n",
    "                         with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                              lines = file.readlines()\n",
    "                              for line in lines:\n",
    "                                   parts = line.split()\n",
    "                    \n",
    "                                   if parts[1] == term:\n",
    "                                        doc_id = int(parts[2]) - 1  \n",
    "                                        terms_dictionaries[doc_id].append(term)\n",
    "\n",
    "                    print('Terms dictionaries:', terms_dictionaries)\n",
    "\n",
    "                    for i in range(N):\n",
    "                        query_modified = query\n",
    "                        query_terms_without_stemming = [term for term in query.split() if term.lower() not in operators]\n",
    "                        \n",
    "                        for term in query_terms_without_stemming:\n",
    "                              #print(term)\n",
    "                              #print('\\n')\n",
    "                              # Replacing terms with 'True' or 'False' based on their presence in the document\n",
    "                            \n",
    "                              if Porter.stem(term) in terms_dictionaries[i]:\n",
    "                                   query_modified = query_modified.replace(term, 'True')\n",
    "                              else:\n",
    "                                   query_modified = query_modified.replace(term, 'False')\n",
    "    \n",
    "                        query_modified = query_modified.replace('OR', 'or')\n",
    "                        query_modified = query_modified.replace('AND', 'and')\n",
    "                        query_modified = query_modified.replace('NOT', 'not')\n",
    "\n",
    "                        print(f\"Document {i + 1} Final Query for Simplification: {query_modified}\")\n",
    "\n",
    "                        \n",
    "\n",
    "                        try:\n",
    "                            boolean_dict[i] = eval(query_modified)\n",
    "                        except Exception as e:\n",
    "                           print(f\"Error evaluating query for document {i + 1}: {e}\")\n",
    "                           boolean_dict[i] = False  \n",
    "\n",
    "                    print('Boolean Dictionary:', boolean_dict)            \n",
    "                                \n",
    "                case 'Lancaster':\n",
    "                    if not validate_logic_query(query):\n",
    "                        return None\n",
    "\n",
    "                    \n",
    "                    file_path = 'inverse_split_lancaster.txt'\n",
    "                    query_terms = list([Lancaster.stem(term) for term in query.split() if term.lower() not in StopWords])                     \n",
    "                    print('Query terms:', query_terms)\n",
    "\n",
    "                    # Extracting query terms without operators\n",
    "                    query_terms_without_operators = [term for term in query_terms if term not in ['and', 'or', 'not']]\n",
    "                    print('Terms without operators:', query_terms_without_operators)\n",
    "\n",
    "                    # Initialize variables\n",
    "                    operators = ['and', 'or', 'not']\n",
    "                    terms_dictionaries = [[] for _ in range(N)] \n",
    "\n",
    "                    # looping through the terms of the query to calculate for each document the terms of the query that it contains\n",
    "                    for term in query_terms_without_operators:\n",
    "                         with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                              lines = file.readlines()\n",
    "                              for line in lines:\n",
    "                                   parts = line.split()\n",
    "                    \n",
    "                                   if parts[1] == term:\n",
    "                                        doc_id = int(parts[2]) - 1  \n",
    "                                        terms_dictionaries[doc_id].append(term)\n",
    "\n",
    "                    print('Terms dictionaries:', terms_dictionaries)\n",
    "\n",
    "                    for i in range(N):\n",
    "                        query_modified = query\n",
    "                        query_terms_without_stemming = [term for term in query.split() if term.lower() not in operators]\n",
    "                        \n",
    "                        for term in query_terms_without_stemming:\n",
    "                              #print(term)\n",
    "                              #print('\\n')\n",
    "                              # Replacing terms with 'True' or 'False' based on their presence in the document\n",
    "                            \n",
    "                              if Lancaster.stem(term) in terms_dictionaries[i]:\n",
    "                                   query_modified = query_modified.replace(term, 'True')\n",
    "                              else:\n",
    "                                   query_modified = query_modified.replace(term, 'False')\n",
    "    \n",
    "                        query_modified = query_modified.replace('OR', 'or')\n",
    "                        query_modified = query_modified.replace('AND', 'and')\n",
    "                        query_modified = query_modified.replace('NOT', 'not')\n",
    "\n",
    "                        print(f\"Document {i + 1} Final Query for Simplification: {query_modified}\")\n",
    "\n",
    "                        \n",
    "\n",
    "                        try:\n",
    "                            boolean_dict[i] = eval(query_modified)\n",
    "                        except Exception as e:\n",
    "                           print(f\"Error evaluating query for document {i + 1}: {e}\")\n",
    "                           boolean_dict[i] = False  \n",
    "\n",
    "                    print('Boolean Dictionary:', boolean_dict)             \n",
    "    else:\n",
    "            match stemming:\n",
    "                case 'No Stemming':\n",
    "                    if not validate_logic_query(query):\n",
    "                        return None\n",
    "\n",
    "                    \n",
    "                    file_path = 'inverse_reg.txt'\n",
    "                    query_terms = list([term for term in ExpReg.tokenize(query) if term.lower() not in StopWords])                     \n",
    "                    print('Query terms:', query_terms)\n",
    "\n",
    "                    # Extracting query terms without operators\n",
    "                    query_terms_without_operators = [term for term in query_terms if term not in ['and', 'or', 'not']]\n",
    "                    print('Terms without operators:', query_terms_without_operators)\n",
    "\n",
    "                    # Initialize variables\n",
    "                    operators = ['and', 'or', 'not']\n",
    "                    terms_dictionaries = [[] for _ in range(N)] \n",
    "\n",
    "                    # looping through the terms of the query to calculate for each document the terms of the query that it contains\n",
    "                    for term in query_terms_without_operators:\n",
    "                         with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                              lines = file.readlines()\n",
    "                              for line in lines:\n",
    "                                   parts = line.split()\n",
    "                    \n",
    "                                   if parts[1] == term:\n",
    "                                        doc_id = int(parts[2]) - 1  \n",
    "                                        terms_dictionaries[doc_id].append(term)\n",
    "\n",
    "                    print('Terms dictionaries:', terms_dictionaries)\n",
    "\n",
    "                    for i in range(N):\n",
    "                        query_modified = query\n",
    "                        query_terms_without_stemming = [term for term in ExpReg.tokenize(query) if term.lower() not in operators]\n",
    "                        \n",
    "                        for term in query_terms_without_stemming:\n",
    "                              #print(term)\n",
    "                              #print('\\n')\n",
    "                              # Replacing terms with 'True' or 'False' based on their presence in the document\n",
    "                            \n",
    "                              if term in terms_dictionaries[i]:\n",
    "                                   query_modified = query_modified.replace(term, 'True')\n",
    "                              else:\n",
    "                                   query_modified = query_modified.replace(term, 'False')\n",
    "    \n",
    "                        query_modified = query_modified.replace('OR', 'or')\n",
    "                        query_modified = query_modified.replace('AND', 'and')\n",
    "                        query_modified = query_modified.replace('NOT', 'not')\n",
    "\n",
    "                        print(f\"Document {i + 1} Final Query for Simplification: {query_modified}\")\n",
    "\n",
    "                        \n",
    "\n",
    "                        try:\n",
    "                            boolean_dict[i] = eval(query_modified)\n",
    "                        except Exception as e:\n",
    "                           print(f\"Error evaluating query for document {i + 1}: {e}\")\n",
    "                           boolean_dict[i] = False  \n",
    "\n",
    "                    print('Boolean Dictionary:', boolean_dict)            \n",
    "                case 'Porter':\n",
    "\n",
    "                    \n",
    "                    if not validate_logic_query(query):\n",
    "                        return None\n",
    "\n",
    "                    \n",
    "                    file_path = 'inverse_reg_porter.txt'\n",
    "                    query_terms = list([Porter.stem(term) for term in ExpReg.tokenize(query) if term.lower() not in StopWords])                     \n",
    "                    print('Query terms:', query_terms)\n",
    "\n",
    "                    # Extracting query terms without operators\n",
    "                    query_terms_without_operators = [term for term in query_terms if term not in ['and', 'or', 'not']]\n",
    "                    print('Terms without operators:', query_terms_without_operators)\n",
    "\n",
    "                    # Initialize variables\n",
    "                    operators = ['and', 'or', 'not']\n",
    "                    terms_dictionaries = [[] for _ in range(N)] \n",
    "\n",
    "                    # looping through the terms of the query to calculate for each document the terms of the query that it contains\n",
    "                    for term in query_terms_without_operators:\n",
    "                         with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                              lines = file.readlines()\n",
    "                              for line in lines:\n",
    "                                   parts = line.split()\n",
    "                    \n",
    "                                   if parts[1] == term:\n",
    "                                        doc_id = int(parts[2]) - 1  \n",
    "                                        terms_dictionaries[doc_id].append(term)\n",
    "\n",
    "                    print('Terms dictionaries:', terms_dictionaries)\n",
    "\n",
    "                    for i in range(N):\n",
    "                        query_modified = query\n",
    "                        query_terms_without_stemming = [term for term in ExpReg.tokenize(query) if term.lower() not in operators]\n",
    "                        \n",
    "                        for term in query_terms_without_stemming:\n",
    "                              #print(term)\n",
    "                              #print('\\n')\n",
    "                              # Replacing terms with 'True' or 'False' based on their presence in the document\n",
    "                            \n",
    "                              if Porter.stem(term) in terms_dictionaries[i]:\n",
    "                                   query_modified = query_modified.replace(term, 'True')\n",
    "                              else:\n",
    "                                   query_modified = query_modified.replace(term, 'False')\n",
    "    \n",
    "                        query_modified = query_modified.replace('OR', 'or')\n",
    "                        query_modified = query_modified.replace('AND', 'and')\n",
    "                        query_modified = query_modified.replace('NOT', 'not')\n",
    "\n",
    "                        print(f\"Document {i + 1} Final Query for Simplification: {query_modified}\")\n",
    "\n",
    "                        \n",
    "\n",
    "                        try:\n",
    "                            boolean_dict[i+1] = eval(query_modified)\n",
    "                        except Exception as e:\n",
    "                           print(f\"Error evaluating query for document {i + 1}: {e}\")\n",
    "                           boolean_dict[i+1] = False  \n",
    "\n",
    "                    print('Boolean Dictionary:', boolean_dict)\n",
    "                    \n",
    "                 \n",
    "                case 'Lancaster':\n",
    "                    if not validate_logic_query(query):\n",
    "                        return None\n",
    "\n",
    "                    \n",
    "                    file_path = 'inverse_reg_lancaster.txt'\n",
    "                    query_terms = list([Lancaster.stem(term) for term in ExpReg.tokenize(query) if term.lower() not in StopWords])                     \n",
    "                    print('Query terms:', query_terms)\n",
    "\n",
    "                    # Extracting query terms without operators\n",
    "                    query_terms_without_operators = [term for term in query_terms if term not in ['and', 'or', 'not']]\n",
    "                    print('Terms without operators:', query_terms_without_operators)\n",
    "\n",
    "                    # Initialize variables\n",
    "                    operators = ['and', 'or', 'not']\n",
    "                    terms_dictionaries = [[] for _ in range(N)] \n",
    "\n",
    "                    # looping through the terms of the query to calculate for each document the terms of the query that it contains\n",
    "                    for term in query_terms_without_operators:\n",
    "                         with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                              lines = file.readlines()\n",
    "                              for line in lines:\n",
    "                                   parts = line.split()\n",
    "                    \n",
    "                                   if parts[1] == term:\n",
    "                                        doc_id = int(parts[2]) - 1  \n",
    "                                        terms_dictionaries[doc_id].append(term)\n",
    "\n",
    "                    print('Terms dictionaries:', terms_dictionaries)\n",
    "\n",
    "                    for i in range(N):\n",
    "                        query_modified = query\n",
    "                        query_terms_without_stemming = [term for term in ExpReg.tokenize(query) if term.lower() not in operators]\n",
    "                        \n",
    "                        for term in query_terms_without_stemming:\n",
    "                              #print(term)\n",
    "                              #print('\\n')\n",
    "                              # Replacing terms with 'True' or 'False' based on their presence in the document\n",
    "                            \n",
    "                              if Lancaster.stem(term) in terms_dictionaries[i]:\n",
    "                                   query_modified = query_modified.replace(term, 'True')\n",
    "                              else:\n",
    "                                   query_modified = query_modified.replace(term, 'False')\n",
    "    \n",
    "                        query_modified = query_modified.replace('OR', 'or')\n",
    "                        query_modified = query_modified.replace('AND', 'and')\n",
    "                        query_modified = query_modified.replace('NOT', 'not')\n",
    "\n",
    "                        print(f\"Document {i + 1} Final Query for Simplification: {query_modified}\")\n",
    "\n",
    "                        \n",
    "\n",
    "                        try:\n",
    "                            boolean_dict[i+1] = eval(query_modified)\n",
    "                        except Exception as e:\n",
    "                           print(f\"Error evaluating query for document {i + 1}: {e}\")\n",
    "                           boolean_dict[i+1] = False  \n",
    "\n",
    "                    print('Boolean Dictionary:', boolean_dict)           \n",
    "\n",
    "    return boolean_dict\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query terms: ['chatgpt', 'gpt-3.5']\n",
      "Terms without operators: ['chatgpt', 'gpt-3.5']\n",
      "Terms dictionaries: [[], ['chatgpt'], [], [], [], ['gpt-3.5']]\n",
      "Document 1 Final Query for Simplification: False or False\n",
      "Document 2 Final Query for Simplification: True or False\n",
      "Document 3 Final Query for Simplification: False or False\n",
      "Document 4 Final Query for Simplification: False or False\n",
      "Document 5 Final Query for Simplification: False or False\n",
      "Document 6 Final Query for Simplification: False or True\n",
      "Boolean Dictionary: {1: False, 2: True, 3: False, 4: False, 5: False, 6: True}\n"
     ]
    }
   ],
   "source": [
    "my_dict = boolean_model('ChatGPT OR GPT-3.5','Porter','Reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def model_evaluation(query,model_results):\n",
    "\n",
    "    model_returned_docs = list(model_results.keys())\n",
    "    real_relevant_docs = []\n",
    "    query_id = 0\n",
    "    \n",
    "    print(model_returned_docs)\n",
    "    file_path = 'Queries.txt'\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        cpt = 0\n",
    "        for line in lines:\n",
    "            cpt += 1\n",
    "            if line == query:\n",
    "                query_id = cpt\n",
    "                break\n",
    "    \n",
    "    print(query_id)\n",
    "    file_path = 'Judgements.txt'\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            parts = line.split()\n",
    "            if int(parts[0]) == query_id:\n",
    "                real_relevant_docs.append(parts[1])\n",
    "    \n",
    "    print(real_relevant_docs)\n",
    "    \n",
    "    selected_relevant_docs = 0\n",
    "    selected_relevant_docs_rang_5 = 0\n",
    "    selected_relevant_docs_rang_10 = 0\n",
    "\n",
    "    for i,doc in enumerate(model_returned_docs):\n",
    "\n",
    "        if i < 5:\n",
    "            if doc in real_relevant_docs:\n",
    "                selected_relevant_docs_rang_5 += 1\n",
    "                selected_relevant_docs_rang_10 += 1\n",
    "                selected_relevant_docs += 1\n",
    "        elif i < 10:\n",
    "            if doc in real_relevant_docs:\n",
    "                selected_relevant_docs_rang_10 += 1\n",
    "                selected_relevant_docs += 1\n",
    "        else:\n",
    "            if doc in real_relevant_docs:\n",
    "                selected_relevant_docs += 1\n",
    "    \n",
    "    print('selected_relevant_docs:',selected_relevant_docs)\n",
    "    p = selected_relevant_docs / len(model_returned_docs)\n",
    "\n",
    "    p5 = selected_relevant_docs_rang_5 / 5\n",
    "\n",
    "    p10 = selected_relevant_docs_rang_10 / 10\n",
    "\n",
    "    r = selected_relevant_docs / len(real_relevant_docs)\n",
    "\n",
    "    F_score = ( 2 * p * r ) / ( p + r)\n",
    "\n",
    "    recalls = []\n",
    "    precesions = []\n",
    "    selected_relevant_docs = 0\n",
    "    print('len(real_relevant_docs)',len(real_relevant_docs))\n",
    "    for i,doc in enumerate(model_returned_docs):\n",
    "        \n",
    "        if doc in real_relevant_docs:\n",
    "            selected_relevant_docs += 1\n",
    "        print('i + 1 ', i + 1 )\n",
    "        print('selected_relevant_docs' , selected_relevant_docs)\n",
    "        p = selected_relevant_docs / ( i + 1 )\n",
    "        r = selected_relevant_docs / len(real_relevant_docs)\n",
    "        precesions.append(p)\n",
    "        recalls.append(r)\n",
    "    \n",
    "    #interpolation\n",
    "    \n",
    "    \n",
    "    recalls2 = [0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "\n",
    "\n",
    "    precisions2 = []\n",
    "    for r2 in recalls2:\n",
    "        max_precision = 0\n",
    "        for r, p in zip(recalls, precesions):\n",
    "                if r >= r2:\n",
    "                    max_precision = max(max_precision, p)\n",
    "        precisions2.append(max_precision)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return p , p5 , p10 , r ,  F_score , precesions , recalls , precisions2 , recalls2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'4': 0.6029509020069551,\n",
       " '1': 0.3183520069376301,\n",
       " '6': 0.23063725500178212,\n",
       " '5': 0.13264666955734586,\n",
       " '2': 0.06632333477867293}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'What recent studies explore query expansion through embeddings?'\n",
    "my_dict = rsv(query,'Porter','Reg')\n",
    "my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4', '1', '6', '5', '2']\n",
      "2\n",
      "['2', '6']\n",
      "selected_relevant_docs: 2\n",
      "len(real_relevant_docs) 2\n",
      "i + 1  1\n",
      "selected_relevant_docs 0\n",
      "i + 1  2\n",
      "selected_relevant_docs 0\n",
      "i + 1  3\n",
      "selected_relevant_docs 1\n",
      "i + 1  4\n",
      "selected_relevant_docs 1\n",
      "i + 1  5\n",
      "selected_relevant_docs 2\n",
      "precision = 0.4 , p@5 = 0.4 , p@10 = 0.2 , recall = 1.0 , F_score = 0.5714285714285715\n",
      "precesions [0.0, 0.0, 0.3333333333333333, 0.25, 0.4]\n",
      "recalls [0.0, 0.0, 0.5, 0.5, 1.0]\n",
      "interpolation\n",
      "precesions2 [0.4, 0.4, 0.4, 0.4, 0.4, 0.4]\n",
      "recalls2 [0, 0.2, 0.4, 0.6, 0.8, 1.0]\n"
     ]
    }
   ],
   "source": [
    "p , p5 , p10 , r ,  F_score , precesions , recalls , precisions2 , recalls2 = model_evaluation(query,my_dict)\n",
    "print(f'precision = {p} , p@5 = {p5} , p@10 = {p10} , recall = {r} , F_score = {F_score}')\n",
    "print('precesions',precesions)\n",
    "print('recalls',recalls)\n",
    "print('interpolation')\n",
    "print('precesions2',precisions2)\n",
    "print('recalls2',recalls2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_plot(recalls, precesions):\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.title(f\"Curve Recall/Precision\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.plot(recalls, precesions, color='teal', marker='o')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_plot(recalls, precesions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_plot(recalls2, precisions2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Function to generate and display the plot inside an Output widget\n",
    "def show_plot2(recalls, precisions, output_widget):\n",
    "    with output_widget:\n",
    "        clear_output(wait=True)  \n",
    "        plt.figure(figsize=(5, 3))\n",
    "        plt.title(\"Curve Recall/Precision\")\n",
    "        plt.xlabel(\"Recall\")\n",
    "        plt.ylabel(\"Precision\")\n",
    "        plt.plot(recalls, precisions, color='teal', marker='o')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "817fdc63823842d29d86c14e38ff5ff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Text(value='', description='Query:', layout=Layout(width='65%'), placeholder='Enter your query …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "931d42678317497290e62d5cdc7c4a1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Preprocessing:', layout=Layout(width='45%'), options=('Spl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "253c86b755244c18a916a278d2652c46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', description='Results:', layout=Layout(height='300px', width='100%'), placeholder='Results w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "474c9e4dff3a49b9b19f8073b93f7ec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', description='Results:', layout=Layout(height='50px', width='100%'), placeholder='Results wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "debcd33a7e1f44ce8172bc34a10ee588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4', '1', '6', '5', '2']\n",
      "2\n",
      "['2', '6']\n",
      "selected_relevant_docs: 2\n",
      "len(real_relevant_docs) 2\n",
      "i + 1  1\n",
      "selected_relevant_docs 0\n",
      "i + 1  2\n",
      "selected_relevant_docs 0\n",
      "i + 1  3\n",
      "selected_relevant_docs 1\n",
      "i + 1  4\n",
      "selected_relevant_docs 1\n",
      "i + 1  5\n",
      "selected_relevant_docs 2\n"
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "border_color = '#ccc'\n",
    "background_color = '#f9f9f9'\n",
    "container_background_color = '#f0f0f0'\n",
    "\n",
    "# Query input with adjusted width\n",
    "query_input = widgets.Text(\n",
    "    description='Query:',\n",
    "    placeholder='Enter your query here',\n",
    "    layout=widgets.Layout(width='65%')\n",
    ")\n",
    "\n",
    "# Submit button with adjusted width\n",
    "submit_button = widgets.Button(\n",
    "    description='Search',\n",
    "    layout=widgets.Layout(width='15%')\n",
    ")\n",
    "\n",
    "# Combine query input and submit button in a single line with styling\n",
    "query_container = widgets.HBox(\n",
    "    [query_input, submit_button],\n",
    "    layout=widgets.Layout(\n",
    "        border=f'2px solid {border_color}',\n",
    "        border_radius='10px',\n",
    "        padding='10px',\n",
    "        background_color=background_color,\n",
    "        width='80%'  # Adjust container width\n",
    "    )\n",
    ")\n",
    "\n",
    "# Dropdowns for preprocessing and stemming parameters\n",
    "preprocessing_options = ['Split', 'Reg']\n",
    "stemming_options = ['Porter', 'Lancaster', 'No Stemming']\n",
    "\n",
    "preprocessing_dropdown = widgets.Dropdown(\n",
    "    options=preprocessing_options,\n",
    "    description='Preprocessing:',\n",
    "    layout=widgets.Layout(width='45%')\n",
    ")\n",
    "\n",
    "stemming_dropdown = widgets.Dropdown(\n",
    "    options=stemming_options,\n",
    "    description='Stemming:',\n",
    "    layout=widgets.Layout(width='45%')\n",
    ")\n",
    "\n",
    "# Group preprocessing and stemming dropdowns in a container\n",
    "preprocessing_container = widgets.HBox(\n",
    "    [preprocessing_dropdown, stemming_dropdown],\n",
    "    layout=widgets.Layout(\n",
    "        border=f'2px solid {border_color}',\n",
    "        border_radius='10px',\n",
    "        padding='10px',\n",
    "        background_color=container_background_color,\n",
    "        justify_content='space-between',\n",
    "        width='80%'  # Adjust container width\n",
    "    )\n",
    ")\n",
    "\n",
    "# RadioButtons for file selection with toggle button\n",
    "file_options = ['DOCS per Term', 'Terms per Doc']\n",
    "file_selection = widgets.RadioButtons(\n",
    "    options=file_options,\n",
    "    description='File Type:',\n",
    "    layout=widgets.Layout(width='50%')\n",
    ")\n",
    "\n",
    "toggle_button = widgets.ToggleButton(\n",
    "    value=False,\n",
    "    description='Enable/Disable File Selection',\n",
    "    button_style='', \n",
    "    tooltip='Click to enable/disable the file selection widget',\n",
    "    layout=widgets.Layout(width='45%')\n",
    ")\n",
    "\n",
    "# Combine toggle button and file selection radio buttons\n",
    "file_selection_container = widgets.HBox(\n",
    "    [toggle_button, file_selection],\n",
    "    layout=widgets.Layout(\n",
    "        border=f'2px solid {border_color}',\n",
    "        border_radius='10px',\n",
    "        padding='10px',\n",
    "        background_color=background_color,\n",
    "        justify_content='space-between',\n",
    "        width='80%'  # Adjust container width\n",
    "    )\n",
    ")\n",
    "\n",
    "# Toggle function to enable/disable the file selection\n",
    "def on_toggle_button_change(change):\n",
    "    file_selection.disabled = not change['new']  \n",
    "\n",
    "toggle_button.observe(on_toggle_button_change, names='value')\n",
    "\n",
    "def is_file_selection_enabled():\n",
    "    return not file_selection.disabled\n",
    "\n",
    "# RadioButtons for model selection\n",
    "models_options = ['Scalar Product', 'Cosine Measure', 'Jaccard Measure','BM25','Boolean']\n",
    "model_selection = widgets.RadioButtons(\n",
    "    options=models_options,\n",
    "    description='Vector space model:',\n",
    "    layout=widgets.Layout(width='80%')  # Adjust width to match other elements\n",
    ")\n",
    "\n",
    "k_input = widgets.Text(\n",
    "    description='K:',\n",
    "    placeholder='',\n",
    "    layout=widgets.Layout(width='65%')\n",
    ")\n",
    "\n",
    "b_input = widgets.Text(\n",
    "    description='B:',\n",
    "    placeholder='',\n",
    "    layout=widgets.Layout(width='65%')\n",
    ")\n",
    "\n",
    "params_container = widgets.VBox(\n",
    "    [k_input, b_input],\n",
    "    layout=widgets.Layout(\n",
    "        border=f'2px solid {border_color}',\n",
    "        border_radius='10px',\n",
    "        padding='15px',\n",
    "        background_color=background_color,\n",
    "        width='85%'  # Adjust container width\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# Group dropdowns and model selection in a single container\n",
    "dropdowns_container = widgets.VBox(\n",
    "    [preprocessing_container, file_selection_container, model_selection, params_container],\n",
    "    layout=widgets.Layout(\n",
    "        border=f'2px solid {border_color}',\n",
    "        border_radius='10px',\n",
    "        padding='15px',\n",
    "        background_color=background_color,\n",
    "        width='85%'  # Adjust container width\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Create a text area to display results\n",
    "result_area = widgets.Textarea(\n",
    "    description='Results:',\n",
    "    placeholder='Results will be displayed here',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(\n",
    "        width='100%',\n",
    "        height='300px',\n",
    "    )\n",
    ")\n",
    "\n",
    "result_area2 = widgets.Textarea(\n",
    "    description='Results:',\n",
    "    placeholder='Results will be displayed here',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(\n",
    "        width='100%',\n",
    "        height='50px',\n",
    "    )\n",
    ")\n",
    "\n",
    "plot_output = widgets.Output()\n",
    "\n",
    "# Function to handle button click\n",
    "def on_submit(b):\n",
    "    query = query_input.value\n",
    "    preprocessing = preprocessing_dropdown.value\n",
    "    stemming = stemming_dropdown.value\n",
    "    selected_file = file_selection.value\n",
    "    model = model_selection.value\n",
    "    result_content = \"\"\n",
    "    if k_input.value == '':\n",
    "        k = 0\n",
    "    else:\n",
    "        k = float(k_input.value)\n",
    "    \n",
    "    if b_input.value == '':\n",
    "        b = 0\n",
    "    else:\n",
    "        b = float(k_input.value)\n",
    "\n",
    "\n",
    "    if is_file_selection_enabled():\n",
    "        file_path = \"\"\n",
    "        if selected_file == \"Terms per Doc\":  # descriptive file\n",
    "            if preprocessing == \"Split\":\n",
    "                match stemming:\n",
    "                    case 'No Stemming':\n",
    "                        file_path = 'descripteur_split.txt'\n",
    "                    case 'Porter':\n",
    "                        file_path = 'descripteur_split_porter.txt'\n",
    "                    case 'Lancaster':\n",
    "                        file_path = 'descripteur_split_lancaster.txt'\n",
    "            else:\n",
    "                match stemming:\n",
    "                    case 'No Stemming':\n",
    "                        file_path = 'descripteur_reg.txt'\n",
    "                    case 'Porter':\n",
    "                        file_path = 'descripteur_reg_porter.txt'\n",
    "                    case 'Lancaster':\n",
    "                        file_path = 'descripteur_reg_lancaster.txt'\n",
    "        else:  # inverse file\n",
    "            if preprocessing == \"Split\":\n",
    "                match stemming:\n",
    "                    case 'No Stemming':\n",
    "                        file_path = 'inverse_split.txt'\n",
    "                    case 'Porter':\n",
    "                        file_path = 'inverse_split_porter.txt'\n",
    "                        query = Porter.stem(query)\n",
    "                    case 'Lancaster':\n",
    "                        file_path = 'inverse_split_lancaster.txt'\n",
    "                        query = Lancaster.stem(query)\n",
    "            else:\n",
    "                match stemming:\n",
    "                    case 'No Stemming':\n",
    "                        file_path = 'inverse_reg.txt'\n",
    "                    case 'Porter':\n",
    "                        file_path = 'inverse_reg_porter.txt'\n",
    "                        query = Porter.stem(query)\n",
    "                        mdict = rsv(query,'Porter','Reg')\n",
    "                        print(mdict)\n",
    "                        for doc,v in mdict.items():\n",
    "                            formatted_line = f\"{doc:<3} {v:<3}\\n\"\n",
    "                            result_content += formatted_line\n",
    "                    case 'Lancaster':\n",
    "                        file_path = 'inverse_reg_lancaster.txt'\n",
    "                        query = Lancaster.stem(query)\n",
    "\n",
    "        # Initialize result content with the correct header and numbered lines\n",
    "        line_counter = 1\n",
    "        if selected_file == \"Terms per Doc\":\n",
    "            result_content = \"N  Ndoc   Term           Freq   Weight  Positions\\n\"\n",
    "        \n",
    "            # Track terms and frequencies for the selected document\n",
    "            term_count = 0\n",
    "        \n",
    "        \n",
    "             # Filter the file content based on query and file type\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                lines = file.readlines()\n",
    "                for line in lines:\n",
    "                    parts = line.split()\n",
    "                    if parts[0] == query:\n",
    "                        # Format and add the numbered line to result_content\n",
    "                        formatted_line = f\"{line_counter:<3} {parts[0]:<5} {parts[1]:<15} {parts[2]:<5} {parts[3]:<10} {parts[4]:<10}\\n\"\n",
    "                        result_content += formatted_line\n",
    "                        line_counter += 1\n",
    "                        term_count += int(parts[2])  # Increment by term frequency\n",
    "                \n",
    "            # Append document vocabulary and size\n",
    "            result_content += f\"-------------------------------------------------------------------\"\n",
    "            result_content += f\"\\n# Doc vocabulary: {line_counter-1}               \"\n",
    "            result_content += f\"# Doc size: {term_count}\\n\"\n",
    "    \n",
    "        else:\n",
    "            result_content = \"N   Term            Ndoc   Freq   Weight\\n\"\n",
    "        \n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                lines = file.readlines()\n",
    "                for line in lines:\n",
    "                    parts = line.split()\n",
    "                    if parts[1] == query:\n",
    "                        formatted_line = f\"{parts[0]:<3} {parts[1]:<15} {parts[2]:<5} {parts[3]:<5} {parts[4]:<10}\\n\"\n",
    "                        result_content += formatted_line\n",
    "                        line_counter += 1\n",
    "\n",
    "  \n",
    "        if line_counter == 1:  # Only header is present\n",
    "            result_content += \"No matching results found.\"\n",
    "    \n",
    "    else:\n",
    "        if model== 'Scalar Product':\n",
    "            my_dict = rsv(query,stemming,preprocessing)\n",
    "            #print('test')\n",
    "\n",
    "        elif model == 'Cosine Measure':\n",
    "            my_dict = cosine(query,stemming,preprocessing)\n",
    "\n",
    "        elif model == 'Jaccard Measure':\n",
    "            my_dict = jaccard(query,stemming,preprocessing)\n",
    "\n",
    "        elif model == 'BM25':\n",
    "            my_dict = bm25(query,stemming,preprocessing,k,b)\n",
    "        \n",
    "        elif model == 'Boolean':\n",
    "            my_dict = boolean_model(query,stemming,preprocessing)\n",
    "\n",
    "        \n",
    "        if model == 'Boolean':\n",
    "\n",
    "            if my_dict ==None:\n",
    "                result_content = 'query not valid '\n",
    "            else:\n",
    "                for doc,v in my_dict.items():\n",
    "                    formatted_line = f\"{doc:<3} {str(v):<3}\\n\"\n",
    "                    result_content += formatted_line\n",
    "\n",
    "        \n",
    "        else:\n",
    "            for doc,v in my_dict.items():\n",
    "                formatted_line = f\"{doc:<3} {v:<3}\\n\"\n",
    "                result_content += formatted_line\n",
    "\n",
    "        p , p5 , p10 , r ,  F_score , precesions , recalls , precisions2 , recalls2 = model_evaluation(query,my_dict)\n",
    "        result_area2.value = f'precision = {p} , p@5 = {p5} , p@10 = {p10} , recall = {r} , F_score = {F_score}'\n",
    "\n",
    "        if len(recalls2) == len(precisions2):\n",
    "            show_plot2(recalls2, precisions2, plot_output)\n",
    "        else:\n",
    "            with plot_output:\n",
    "                clear_output(wait=True)\n",
    "                print(\"Error: Recall and Precision arrays must have the same length.\")\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    result_area.value = result_content\n",
    "\n",
    "submit_button.on_click(on_submit)\n",
    "\n",
    "# Display all widgets\n",
    "display(query_container, dropdowns_container, result_area, result_area2,plot_output)\n",
    "#information retrieval\n",
    "#Large language models (LLM)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
